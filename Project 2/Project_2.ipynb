{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Project 2</h1></center>\n",
    "<br>\n",
    "<center><font size=\"5\">Name - Spandan Patil</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dependencies version:\n",
    "\n",
    "pandas - 2.2.3\n",
    "numpy - 1.26.4\n",
    "nltk - 3.9.1\n",
    "bs4 - 0.0.2\n",
    "gensim - 4.3.3\n",
    "scikit-learn - 1.6.1\n",
    "torch - 2.5.1+cu124\n",
    "python - 3.12.8\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Spandan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Spandan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Spandan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import gensim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am reading the amazon kitchen products review dataset and storing it into a Dataframe object using Pandas\n",
    "\n",
    "df = pd.read_csv('./amazon_reviews_us_Office_Products_v1_00.tsv', sep='\\t', on_bad_lines=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of all the features in the dataset, I am combining the review_headlines with the review_body column as input feature and taking star_rating as my output label.\n",
    "df[\"review_body\"] = df[\"review_headline\"] + \" \" + df[\"review_body\"]\n",
    "df = df[[\"review_body\", \"star_rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am converting all the values present in the star_rating columns into numeric using the in-built pandas function to_numeric, also by setting the errors parameter to 'coerce' all the values which are not able to be converted to numeric will be set to NaN. In our case these would be mainly the rows having the invalid date values. Hence, we can just drop all the NaN value rows present in the star_ratings column which are be generated using the dropna function our pandas dataframe. After the processing we can see that all the rows present in the dataset are having a star_ratings label from 1 to 5.\n",
    "df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n",
    "df = df.dropna(subset=['star_rating'])\n",
    "df = df.dropna(subset=['review_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here I am creating the sampled df, which consists of randomly sample 50,000 rows from each of the star rating from 1 to 5. I am also shuffling the sampled df at the end.\n",
    "sampled_df = pd.DataFrame(columns=[\"review_body\", \"star_rating\"])\n",
    "\n",
    "r_state=34\n",
    "\n",
    "for r in range(1, 6):\n",
    "    r_sample = df[df[\"star_rating\"] == r]\n",
    "    r_sample = r_sample.sample(n=50000, random_state=r_state)\n",
    "    sampled_df = pd.concat([sampled_df, r_sample], ignore_index=True)\n",
    "\n",
    "sampled_df = sampled_df.sample(frac=1, random_state=r_state).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am freeing up the memory by removing the uwanted variable for better RAM usage.\n",
    "del df\n",
    "del r_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "star_rating\n",
       "5.0    50000\n",
       "1.0    50000\n",
       "4.0    50000\n",
       "3.0    50000\n",
       "2.0    50000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we can see here the sampled df have 50,000 row for each star rating from 1 to 5.\n",
    "sampled_df[\"star_rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the function used to create our sentiment label class - positive(1), negative(2), neutral(3).\n",
    "def create_label(rating):\n",
    "    if rating > 3:\n",
    "        return 1\n",
    "    elif rating == 3:\n",
    "        return 3\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am creating the output label class by applying the create_label function to the star_rating column.\n",
    "sampled_df[\"senti_label\"] = sampled_df[\"star_rating\"].apply(create_label)\n",
    "\n",
    "sampled_df = sampled_df[[\"review_body\", \"senti_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>senti_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Energizer ER-P512 NiMH Cordless Phone Battery ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm very disappointed with this product The pa...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just what I needed! Just what I needed! Hard t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great! I like this lobby dustpan. It works gre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is NOT a high yield cartridge! Even thoug...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  senti_label\n",
       "0  Energizer ER-P512 NiMH Cordless Phone Battery ...            1\n",
       "1  I'm very disappointed with this product The pa...            2\n",
       "2  Just what I needed! Just what I needed! Hard t...            1\n",
       "3  great! I like this lobby dustpan. It works gre...            1\n",
       "4  This is NOT a high yield cartridge! Even thoug...            3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we can see we have our output label class created.\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i have saved the google news 300 word embedding model for better performance.\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "wv.save(\"gensim_model_w2v_gnews_300.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the download and saved google news 300 word embedding model.\n",
    "wv = gensim.models.KeyedVectors.load(\"gensim_model_w2v_gnews_300.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score of great and worst: 0.22344698011875153\n"
     ]
    }
   ],
   "source": [
    "# Checking the first example for google news model, check the similar score of great and worst. As we can see the similar score is very less.\n",
    "similarity_score = wv.similarity(\"great\", \"worst\")\n",
    "print(f\"Similarity Score of great and worst: {similarity_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to him + female = male: [('me', 0.6085236072540283)]\n"
     ]
    }
   ],
   "source": [
    "# We are check the expression him - male + female ~ her, but the google news model is not giving the correct answer.\n",
    "similarity_score = wv.most_similar(positive=[\"him\", \"male\"], negative=[\"female\"], topn=1)\n",
    "print(f\"Most similar to him + female = male: {similarity_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am training my own word embedding model, using the review body column in the dataset, and saving it for better efficiency.\n",
    "\n",
    "my_model = gensim.models.Word2Vec(sentences=sampled_df[\"review_body\"].apply(lambda r: nltk.tokenize.word_tokenize(r)), vector_size=300, window=11, min_count=10)\n",
    "my_model.save(\"gensim_model_w2v_my_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am loading my trained word embedding model, which i have created using review body column in the data. I am only loading the keyVectors instead of the whole model.\n",
    "my_model = gensim.models.Word2Vec.load(\"gensim_model_w2v_my_model.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score of great and worst: 0.2037562131881714\n"
     ]
    }
   ],
   "source": [
    "# Here i am checking the same example again, we can see that the similarity score is even lower than google news model, which is a better estimate.\n",
    "similarity_score = my_model.similarity(\"great\", \"worst\")\n",
    "print(f\"Similarity Score of great and worst: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to him + female = male: [('her', 0.6315240263938904)]\n"
     ]
    }
   ],
   "source": [
    "# Here we can see that the expression him - male + female ~ her is giving correct output. \n",
    "similarity_score = my_model.most_similar(positive=[\"him\", \"female\"], negative=[\"male\"], topn=1)\n",
    "print(f\"Most similar to him + female = male: {similarity_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the two example of 1) calculating the similarity score between great and worst, 2) Checking the validity of expression him - male + femaler ~ her, we can conclude that the word embedding model which we have trains performs better than the pretrained google news 300 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is the function i am using to clean the reviews and do some preprocessing, as i did in HW1, i am not performing lemmanization since it is reducing the accuracy in my case.\n",
    "def clean_review(review):\n",
    "    \n",
    "    # This is the dictionary having all the commonly used contradictions in the lower case format. These include format with ' or without it. For example didn't and didnt both are included.\n",
    "    contractions_dict = {\n",
    "        \"i'm\": \"i am\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"you've\": \"you have\",\n",
    "        \"he's\": \"he has\",\n",
    "        \"she's\": \"she has\",\n",
    "        \"it's\": \"it has\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"here's\": \"here is\",\n",
    "        \"didnt\": \"did not\",\n",
    "        \"cant\": \"cannot\",\n",
    "        \"wont\": \"will not\",\n",
    "        \"dont\": \"do not\",\n",
    "        \"doesnt\": \"does not\",\n",
    "        \"shouldnt\": \"should not\",\n",
    "        \"wouldnt\": \"would not\",\n",
    "        \"mustnt\": \"must not\",\n",
    "        \"neednt\": \"need not\",\n",
    "        \"letd\": \"let us\"\n",
    "    }\n",
    "\n",
    "    # Here i am converting the review to string and then converting them to lowercase using in-built function \n",
    "    review = str(review).lower()\n",
    "    # Here using the Beautiful Soup HTML.PARSER i am extracting only text data from the reviews discarding the html tag or elements.\n",
    "    review = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "    # I am using a simple regex to remove all the URLs from the text.\n",
    "    review = re.sub(r'http\\S+|www\\S+', '', review)\n",
    "    # Here we are going through the contradiction dictionary and replacing each of the contradiction with their expanded form if they are found in the review. \n",
    "    for contraction, expanded in contractions_dict.items():\n",
    "        review = review.replace(contraction, expanded)\n",
    "    # Here i am removing all the non-alphabetic characters execpt for the spaces. This step needs to be done after the contradiction expansion as this will result in the ' getting removed also, we can cause issues when detecting the contradictions.\n",
    "    review = re.sub(r'[^a-zA-Z\\s]', '', review)\n",
    "    # Here i am coverting all the multiple consecutive spaces with single space using a regex.\n",
    "    review = re.sub(r'\\s+', ' ', review)\n",
    "    # Here i am removing the leading and trailing spaces.\n",
    "    review = review.strip()\n",
    "    # I am spliting the review into a list of word based on single space as separator\n",
    "    words = review.split()\n",
    "    # I am filtering out all the stop words and only including the ones which are not present in our stopwords list.\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    # I am joining all the converted word using a single space.\n",
    "    return ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am applying the clean_review function to each review in the review_body column.\n",
    "sampled_df[\"review_body\"] = sampled_df[\"review_body\"].apply(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am tokenizing the cleaned reviews to prepare them for converting into word embeddings.\n",
    "sampled_df['review_body'] = sampled_df['review_body'].apply(lambda r: word_tokenize(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am getting the word embedding for all the words present in the review and taking the mean of each of the dimension to form the sentence embedding.\n",
    "def review_embedding(review, keyVec):\n",
    "    w_vec = [keyVec[w] for w in review if w in keyVec]  # Get word vectors\n",
    "    if len(w_vec) == 0:\n",
    "        return np.zeros(300)  # Return zero vector if no words found\n",
    "    return np.mean(w_vec, axis=0)  # Compute the mean of word vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating two copys of the sampled df to be used for performing embedding via google news 300 modela and my own trained model.\n",
    "g_news_df = sampled_df.copy()\n",
    "my_model_df = sampled_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am performing the sentence emebeding on the review body column with the keyVectors of google news 300 model.\n",
    "g_news_df[\"review_embedding\"] = g_news_df[\"review_body\"].apply(lambda review: review_embedding(review, wv))\n",
    "g_news_df = g_news_df[[\"review_embedding\", \"senti_label\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am performing the sentence emebeding on the review body column with the keyVectors of my own trained model.\n",
    "my_model_df[\"review_embedding\"] = my_model_df[\"review_body\"].apply(lambda review: review_embedding(review, my_model))\n",
    "my_model_df = my_model_df[[\"review_embedding\", \"senti_label\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the binary classification dataset with google new embeddings.\n",
    "binary_g_news_df = g_news_df.copy()\n",
    "binary_g_news_df = binary_g_news_df[binary_g_news_df[\"senti_label\"] != 3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the binary classification dataset with my model embeddings.\n",
    "binary_my_model_df = my_model_df.copy()\n",
    "binary_my_model_df = binary_my_model_df[binary_my_model_df[\"senti_label\"] != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am spliting the dataset into input label X and output label y for binary classification using google news embedding. Also i am converting the 300 dimension to columns, to fit better for the models.\n",
    "b_g_news_X = pd.DataFrame(binary_g_news_df['review_embedding'].tolist(), columns=[f\"d_{i}\" for i in range(300)])\n",
    "b_g_news_y = binary_g_news_df[\"senti_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am spliting the dataset into input label X and output label y for binary classification using my model embedding. Also i am converting the 300 dimension to columns, to fit better for the models.\n",
    "b_my_model_X = pd.DataFrame(binary_my_model_df['review_embedding'].tolist(), columns=[f\"d_{i}\" for i in range(300)])\n",
    "b_my_model_y = binary_my_model_df[\"senti_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am performing train test split for the binary classification using google news embeddings.\n",
    "b_g_news_X_train, b_g_news_X_test, b_g_news_y_train, b_g_news_y_test = train_test_split(b_g_news_X, b_g_news_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am performing train test split for the binary classification using my model embeddings.\n",
    "b_my_model_X_train, b_my_model_X_test, b_my_model_y_train, b_my_model_y_test = train_test_split(b_my_model_X, b_my_model_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Google News 300 Test Accuracy: 0.8220\n"
     ]
    }
   ],
   "source": [
    "# Here I am using percepton model with hyperparamter tuning of increasing the number of epoch to 10000 and enabling early stopping if the accuracy doesn't increase.\n",
    "b_g_news_clf = Perceptron(max_iter=10000, early_stopping=True)\n",
    "b_g_news_clf.fit(b_g_news_X_train, b_g_news_y_train)\n",
    "\n",
    "b_g_news_y_pred = b_g_news_clf.predict(b_g_news_X_test)\n",
    "\n",
    "b_g_news_accuracy_test = accuracy_score(b_g_news_y_test, b_g_news_y_pred)\n",
    "\n",
    "perceptron_g_news_acc = b_g_news_accuracy_test\n",
    "print(f'Perceptron Google News 300 Test Accuracy: {perceptron_g_news_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron My Model Test Accuracy: 0.8330\n"
     ]
    }
   ],
   "source": [
    "# Here I am using percepton model with hyperparamter tuning of increasing the number of epoch to 10000 and enabling early stopping if the accuracy doesn't increase.\n",
    "b_my_model_clf = Perceptron(max_iter=10000, early_stopping=True)\n",
    "b_my_model_clf.fit(b_my_model_X_train, b_my_model_y_train)\n",
    "\n",
    "b_my_model_y_pred = b_my_model_clf.predict(b_my_model_X_test)\n",
    "\n",
    "b_my_model_accuracy_test = accuracy_score(b_my_model_y_test, b_my_model_y_pred)\n",
    "\n",
    "perceptron_my_model_acc = b_my_model_accuracy_test\n",
    "\n",
    "print(f'Perceptron My Model Test Accuracy: {perceptron_my_model_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Perceptron model the Test accuracy - TF-IDF: 0.8864.\n",
    "Hence, we can say that for perceptron model TF-IDF word embedding worked the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Google News 300 Test Accuracy: 0.8454\n"
     ]
    }
   ],
   "source": [
    "# Here I am using LinearSVC model with hyperparamter tuning of increasing the number of epoch to 10000. I am using LinearSVC here instead of SVC due to the large size of data. In the Sklearn documentation, it is given that using SVC may be impratical beyond the tens of thousands of samples, and LinearSVC is preferred for larger datasets.\n",
    "b_g_news_clf = LinearSVC(max_iter=10000)\n",
    "b_g_news_clf.fit(b_g_news_X_train, b_g_news_y_train)\n",
    "\n",
    "b_g_news_y_pred = b_g_news_clf.predict(b_g_news_X_test)\n",
    "\n",
    "b_g_news_accuracy_test = accuracy_score(b_g_news_y_test, b_g_news_y_pred)\n",
    "\n",
    "svm_g_news_acc = b_g_news_accuracy_test\n",
    "\n",
    "print(f'SVM Google News 300 Test Accuracy: {svm_g_news_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM My Model Test Accuracy: 0.8870\n"
     ]
    }
   ],
   "source": [
    "# Here I am using LinearSVC model with hyperparamter tuning of increasing the number of epoch to 10000. I am using LinearSVC here instead of SVC due to the large size of data. In the Sklearn documentation, it is given that using SVC may be impratical beyond the tens of thousands of samples, and LinearSVC is preferred for larger datasets.\n",
    "b_my_model_clf = LinearSVC(max_iter=10000)\n",
    "b_my_model_clf.fit(b_my_model_X_train, b_my_model_y_train)\n",
    "\n",
    "b_my_model_y_pred = b_my_model_clf.predict(b_my_model_X_test)\n",
    "\n",
    "b_my_model_accuracy_test = accuracy_score(b_my_model_y_test, b_my_model_y_pred)\n",
    "\n",
    "svm_my_model_acc = b_my_model_accuracy_test\n",
    "\n",
    "print(f'SVM My Model Test Accuracy: {svm_my_model_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the SVM model the test accurarcy - TF-IDF : 0.9187.\n",
    "Hence, we can say that for SVM model TF-IDF word embedding worked the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Here i am checking if cuda is avaiable and setting the processing device to gpu or cpu.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating a FNN classifier model, based of the instructions given in the pdf, having two hidden layer having 50 and 10 nodes, which are giving in as inputs.\n",
    "class ClassiferModel(nn.Module):\n",
    "    def __init__(self, in_size, h1, h2, out_size):\n",
    "        super(ClassiferModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_size, h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.fc3 = nn.Linear(h2, out_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, r):\n",
    "        r = self.relu(self.fc1(r))  \n",
    "        r = self.relu(self.fc2(r))  \n",
    "        r = self.fc3(r)\n",
    "        return r "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am converting the training and testing input features and output features to tensors, by first converting them into a np.array(). Also for the output labels i am subtracting one from the class as the model need the labels to start from 0.\n",
    "b_g_news_X_train_tensor = torch.tensor(b_g_news_X_train.to_numpy(), dtype=torch.float32)\n",
    "b_g_news_y_train_tensor = torch.tensor(b_g_news_y_train.apply(lambda x: x-1).to_numpy(), dtype=torch.long)\n",
    "b_g_news_X_test_tensor = torch.tensor(b_g_news_X_test.to_numpy(), dtype=torch.float32)\n",
    "b_g_news_y_test_tensor = torch.tensor(b_g_news_y_test.apply(lambda x: x-1).to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the FNN classification model object and pushing it to the device. \n",
    "b_fnnModel = ClassiferModel(300, 50, 10, 2).to(device)\n",
    "\n",
    "# Here i am using cross entropy loss as the loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# Here i am using the Adam optimizer with learning rate as 0.001\n",
    "optimizer = optim.Adam(b_fnnModel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am using the predefined tensordataset and dataloader for training the model in batches.\n",
    "train_dataset = TensorDataset(b_g_news_X_train_tensor, b_g_news_y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.3527926102474332\n",
      "Epoch 2/10, Loss: 0.30803374334722755\n",
      "Epoch 3/10, Loss: 0.2969717775590718\n",
      "Epoch 4/10, Loss: 0.2897937513425946\n",
      "Epoch 5/10, Loss: 0.28395568826943635\n",
      "Epoch 6/10, Loss: 0.2793238676607609\n",
      "Epoch 7/10, Loss: 0.2745317488208413\n",
      "Epoch 8/10, Loss: 0.2705873997010291\n",
      "Epoch 9/10, Loss: 0.26719293488934637\n",
      "Epoch 10/10, Loss: 0.2636081786170602\n"
     ]
    }
   ],
   "source": [
    "# Here i am traing the model for 10 epoches with each batch of size 32.\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    # Setting the model to training mode\n",
    "    b_fnnModel.train().to(device)  \n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Moving the input and output features to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "        optimizer.zero_grad() \n",
    "        outputs = b_fnnModel(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN Binary Classification Google News Accuracy: 87.74%\n"
     ]
    }
   ],
   "source": [
    "# Setting the model to evaluation mode, and calculating the test accuracy.\n",
    "b_fnnModel.eval() \n",
    "with torch.no_grad():\n",
    "    # Moving the input and output features to GPU\n",
    "    X_test_tensor, y_test_tensor = b_g_news_X_test_tensor.to(device), b_g_news_y_test_tensor.to(device)  \n",
    "    outputs = b_fnnModel(X_test_tensor) \n",
    "    _, predicted = torch.max(outputs, 1) \n",
    "    accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu()) \n",
    "\n",
    "b_fnn_g_news_avg_acc = accuracy\n",
    "print(f'FNN Binary Classification Google News Accuracy: {b_fnn_g_news_avg_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del b_fnnModel\n",
    "for inputs, labels in train_loader:\n",
    "    del inputs\n",
    "    del labels\n",
    "del b_g_news_X_test_tensor\n",
    "del b_g_news_y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am converting the training and testing input features and output features to tensors, by first converting them into a np.array(). Also for the output labels i am subtracting one from the class as the model need the labels to start from 0.\n",
    "b_my_model_X_train_tensor = torch.tensor(b_my_model_X_train.to_numpy(), dtype=torch.float32)\n",
    "b_my_model_y_train_tensor = torch.tensor(b_my_model_y_train.apply(lambda x: x-1).to_numpy(), dtype=torch.long)\n",
    "b_my_model_X_test_tensor = torch.tensor(b_my_model_X_test.to_numpy(), dtype=torch.float32)\n",
    "b_my_model_y_test_tensor = torch.tensor(b_my_model_y_test.apply(lambda x: x-1).to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the FNN classification model object and pushing it to the device. \n",
    "b_fnnModel = ClassiferModel(300, 50, 10, 2).to(device)\n",
    "\n",
    "# Here i am using cross entropy loss as the loss function\n",
    "loss_function = nn.CrossEntropyLoss()  \n",
    "# Here i am using the Adam optimizer with learning rate as 0.001\n",
    "optimizer = optim.Adam(b_fnnModel.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am using the predefined tensordataset and dataloader for training the model in batches.\n",
    "train_dataset = TensorDataset(b_my_model_X_train_tensor, b_my_model_y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.27263959611430766\n",
      "Epoch 2/10, Loss: 0.24708786134645344\n",
      "Epoch 3/10, Loss: 0.23792280987277628\n",
      "Epoch 4/10, Loss: 0.2316527714509517\n",
      "Epoch 5/10, Loss: 0.22662251801416278\n",
      "Epoch 6/10, Loss: 0.22271582175157964\n",
      "Epoch 7/10, Loss: 0.21905108323842287\n",
      "Epoch 8/10, Loss: 0.2152104529503733\n",
      "Epoch 9/10, Loss: 0.21267272356562317\n",
      "Epoch 10/10, Loss: 0.21000256772860884\n"
     ]
    }
   ],
   "source": [
    "# Here i am traing the model for 10 epoches with each batch of size 32.\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    # Setting the model to training mode\n",
    "    b_fnnModel.train()  \n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Moving the input and output features to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device) \n",
    "        optimizer.zero_grad() \n",
    "        outputs = b_fnnModel(inputs)  \n",
    "        loss = loss_function(outputs, labels)  \n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN Binary Classification My Model Accuracy: 89.90%\n"
     ]
    }
   ],
   "source": [
    "# Setting the model to evaluation mode, and calculating the test accuracy.\n",
    "b_fnnModel.eval() \n",
    "with torch.no_grad():\n",
    "    # Moving the input and output features to GPU\n",
    "    X_test_tensor, y_test_tensor = b_my_model_X_test_tensor.to(device), b_my_model_y_test_tensor.to(device)  \n",
    "    outputs = b_fnnModel(X_test_tensor)  \n",
    "    _, predicted = torch.max(outputs, 1) \n",
    "    accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu())  \n",
    "\n",
    "b_fnn_my_model_avg_acc = accuracy\n",
    "print(f'FNN Binary Classification My Model Accuracy: {b_fnn_my_model_avg_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "del b_fnnModel\n",
    "for inputs, labels in train_loader:\n",
    "    del inputs\n",
    "    del labels\n",
    "del b_my_model_X_test_tensor\n",
    "del b_my_model_y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Deallocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "del binary_g_news_df\n",
    "del binary_my_model_df\n",
    "del b_g_news_X\n",
    "del b_g_news_y\n",
    "del b_g_news_X_train\n",
    "del b_g_news_X_test\n",
    "del b_g_news_y_train\n",
    "del b_g_news_y_test\n",
    "del b_my_model_X_train\n",
    "del b_my_model_X_test\n",
    "del b_my_model_y_train\n",
    "del b_my_model_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ternary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am spliting the dataset into input label X and output label y for Ternary classification using google news embedding. Also i am converting the 300 dimension to columns, to fit better for the models.\n",
    "t_g_news_X = pd.DataFrame(g_news_df['review_embedding'].tolist(), columns=[f\"d_{i}\" for i in range(300)])\n",
    "t_g_news_y = g_news_df[\"senti_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am performing train test split for the tinary classification using google news embeddings.\n",
    "t_g_news_X_train, t_g_news_X_test, t_g_news_y_train, t_g_news_y_test = train_test_split(t_g_news_X, t_g_news_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am spliting the dataset into input label X and output label y for Ternary classification using my model embedding. Also i am converting the 300 dimension to columns, to fit better for the models.\n",
    "t_my_model_X = pd.DataFrame(my_model_df['review_embedding'].tolist(), columns=[f\"d_{i}\" for i in range(300)])\n",
    "t_my_model_y = my_model_df[\"senti_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am performing train test split for the tinary classification using my model embeddings.\n",
    "t_my_model_X_train, t_my_model_X_test, t_my_model_y_train, t_my_model_y_test = train_test_split(t_my_model_X, t_my_model_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am converting the training and testing input features and output features to tensors, by first converting them into a np.array(). Also for the output labels i am subtracting one from the class as the model need the labels to start from 0.\n",
    "t_g_news_X_train_tensor = torch.tensor(t_g_news_X_train.to_numpy(), dtype=torch.float32)\n",
    "t_g_news_y_train_tensor = torch.tensor(t_g_news_y_train.apply(lambda x: x-1).to_numpy(), dtype=torch.long)\n",
    "t_g_news_X_test_tensor = torch.tensor(t_g_news_X_test.to_numpy(), dtype=torch.float32)\n",
    "t_g_news_y_test_tensor = torch.tensor(t_g_news_y_test.apply(lambda x: x-1).to_numpy(), dtype=torch.long) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the FNN classification model object and pushing it to the device. \n",
    "t_fnnModel = ClassiferModel(300, 50, 10, 3).to(device)\n",
    "# Here i am using cross entropy loss as the loss function\n",
    "loss_function = nn.CrossEntropyLoss()  \n",
    "# Here i am using the Adam optimizer with learning rate as 0.001\n",
    "optimizer = optim.Adam(t_fnnModel.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am using the predefined tensordataset and dataloader for training the model in batches.\n",
    "train_dataset = TensorDataset(t_g_news_X_train_tensor, t_g_news_y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7270138023281097\n",
      "Epoch 2/10, Loss: 0.6616519906377792\n",
      "Epoch 3/10, Loss: 0.6417183092784882\n",
      "Epoch 4/10, Loss: 0.6324936809945106\n",
      "Epoch 5/10, Loss: 0.6248148479747773\n",
      "Epoch 6/10, Loss: 0.6192641370248795\n",
      "Epoch 7/10, Loss: 0.6145317017912865\n",
      "Epoch 8/10, Loss: 0.6104091087841987\n",
      "Epoch 9/10, Loss: 0.6068663216114044\n",
      "Epoch 10/10, Loss: 0.6036971897053719\n"
     ]
    }
   ],
   "source": [
    "# Here i am traing the model for 10 epoches with each batch of size 32.\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    # Setting the model to training mode\n",
    "    t_fnnModel.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Moving the input and output features to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "        optimizer.zero_grad()  \n",
    "        outputs = t_fnnModel(inputs)  \n",
    "        loss = loss_function(outputs, labels)  \n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN Ternary Classification Google News Accuracy: 73.55%\n"
     ]
    }
   ],
   "source": [
    "# Setting the model to evaluation mode, and calculating the test accuracy.\n",
    "t_fnnModel.eval()\n",
    "with torch.no_grad(): \n",
    "     # Moving the input and output features to GPU\n",
    "    X_test_tensor, y_test_tensor = t_g_news_X_test_tensor.to(device), t_g_news_y_test_tensor.to(device)  \n",
    "    outputs = t_fnnModel(X_test_tensor)  \n",
    "    _, predicted = torch.max(outputs, 1) \n",
    "    accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu())\n",
    "\n",
    "t_fnn_g_news_avg_acc = accuracy\n",
    "print(f'FNN Ternary Classification Google News Accuracy: {t_fnn_g_news_avg_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "del t_fnnModel\n",
    "for inputs, labels in train_loader:\n",
    "    del inputs  \n",
    "    del labels\n",
    "del t_g_news_X_test_tensor\n",
    "del t_g_news_y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am converting the training and testing input features and output features to tensors, by first converting them into a np.array(). Also for the output labels i am subtracting one from the class as the model need the labels to start from 0.\n",
    "t_my_model_X_train_tensor = torch.tensor(t_my_model_X_train.to_numpy(), dtype=torch.float32)\n",
    "t_my_model_y_train_tensor = torch.tensor(t_my_model_y_train.apply(lambda x: x-1).to_numpy(), dtype=torch.long)\n",
    "t_my_model_X_test_tensor = torch.tensor(t_my_model_X_test.to_numpy(), dtype=torch.float32)\n",
    "t_my_model_y_test_tensor = torch.tensor(t_my_model_y_test.apply(lambda x: x-1).to_numpy(), dtype=torch.long) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the FNN classification model object and pushing it to the device. \n",
    "t_fnnModel = ClassiferModel(300, 50, 10, 3).to(device)\n",
    "# Here i am using cross entropy loss as the loss function\n",
    "loss_function = nn.CrossEntropyLoss()  \n",
    "# Here i am using the Adam optimizer with learning rate as 0.001\n",
    "optimizer = optim.Adam(t_fnnModel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am using the predefined tensordataset and dataloader for training the model in batches.\n",
    "train_dataset = TensorDataset(t_my_model_X_train_tensor, t_my_model_y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6039382109904289\n",
      "Epoch 2/10, Loss: 0.568785041539669\n",
      "Epoch 3/10, Loss: 0.5591183296942711\n",
      "Epoch 4/10, Loss: 0.5520433605527878\n",
      "Epoch 5/10, Loss: 0.5461914832425118\n",
      "Epoch 6/10, Loss: 0.5422986579918861\n",
      "Epoch 7/10, Loss: 0.5388446628618241\n",
      "Epoch 8/10, Loss: 0.5357346608424187\n",
      "Epoch 9/10, Loss: 0.5327037195944786\n",
      "Epoch 10/10, Loss: 0.5298448876214027\n"
     ]
    }
   ],
   "source": [
    "# Here i am traing the model for 10 epoches with each batch of size 32.\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    # Setting the model to training mode\n",
    "    t_fnnModel.train()  \n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Moving the input and output features to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "        optimizer.zero_grad() \n",
    "        outputs = t_fnnModel(inputs)  \n",
    "        loss = loss_function(outputs, labels)  \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN Ternary Classification My Model Accuracy: 76.39%\n"
     ]
    }
   ],
   "source": [
    "# Setting the model to evaluation mode, and calculating the test accuracy.\n",
    "t_fnnModel.eval()  \n",
    "with torch.no_grad(): \n",
    "    # Moving the input and output features to GPU\n",
    "    X_test_tensor, y_test_tensor = t_my_model_X_test_tensor.to(device), t_my_model_y_test_tensor.to(device)  \n",
    "    outputs = t_fnnModel(X_test_tensor)  \n",
    "    _, predicted = torch.max(outputs, 1)  \n",
    "    accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu())  \n",
    "\n",
    "t_fnn_my_model_avg_acc = accuracy\n",
    "print(f'FNN Ternary Classification My Model Accuracy: {t_fnn_my_model_avg_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "del t_fnnModel\n",
    "for inputs, labels in train_loader:\n",
    "    del inputs \n",
    "    del labels\n",
    "del t_my_model_X_test_tensor\n",
    "del t_my_model_y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Deallocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "del g_news_df\n",
    "del my_model_df\n",
    "del t_g_news_X\n",
    "del t_g_news_y\n",
    "del t_g_news_X_train\n",
    "del t_g_news_X_test\n",
    "del t_g_news_y_train\n",
    "del t_g_news_y_test\n",
    "del t_my_model_X_train\n",
    "del t_my_model_X_test\n",
    "del t_my_model_y_train\n",
    "del t_my_model_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating two copies of sampled df for google news embedding and my model embedding. Here instead of average, we are concatenating first 10 word embeddings\n",
    "g_news_concat_df = sampled_df.copy()\n",
    "my_model_concat_df = sampled_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts the word to word embedding and creates a flatten word embedding vector of first 10 words.\n",
    "def review_concat_embedding(review, keyVec): \n",
    "    # Initialize the flatten first 10 word embedding vector.\n",
    "    res = np.zeros(3000)\n",
    "    \n",
    "    # Getting the word embedding for the first 10 words.\n",
    "    w_vec = []\n",
    "    for w in review:\n",
    "        if w in keyVec:\n",
    "            w_vec.append(keyVec[w])\n",
    "        if len(w_vec) == 10:  # Stop once we have 10 vectors\n",
    "            break\n",
    "    \n",
    "    # Adding them to the final vector, using the start and end indices.\n",
    "    for ind, vec in enumerate(w_vec):\n",
    "        start_ind = ind * 300\n",
    "        res[start_ind:start_ind + 300] = vec\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am performing the sentence emebeding on the review body column with the keyVectors of google news 300 model.\n",
    "g_news_concat_df[\"review_embedding\"] = g_news_concat_df[\"review_body\"].apply(lambda review: review_concat_embedding(review, wv))\n",
    "g_news_concat_df = g_news_concat_df[[\"review_embedding\", \"senti_label\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am performing the sentence emebeding on the review body column with the keyVectors of my model.\n",
    "my_model_concat_df[\"review_embedding\"] = my_model_concat_df[\"review_body\"].apply(lambda review: review_concat_embedding(review, my_model))\n",
    "my_model_concat_df = my_model_concat_df[[\"review_embedding\", \"senti_label\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deallocating these model for efficent RAM usage.\n",
    "del wv\n",
    "del my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the binary classification dataset with google news embeddings.\n",
    "binary_g_news_concat_df = g_news_concat_df.copy()\n",
    "binary_g_news_concat_df = binary_g_news_concat_df[binary_g_news_concat_df[\"senti_label\"] != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the binary classification dataset with my model embeddings.\n",
    "binary_my_model_concat_df = my_model_concat_df.copy()\n",
    "binary_my_model_concat_df = binary_my_model_concat_df[binary_my_model_concat_df[\"senti_label\"] != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the input and output features for binary classification using google news embeddding.\n",
    "b_g_news_concat_X = binary_g_news_concat_df['review_embedding']\n",
    "b_g_news_concat_y = binary_g_news_concat_df[\"senti_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the input and output features for binary classification using my model embeddding.\n",
    "b_my_model_concat_X = binary_my_model_concat_df['review_embedding']\n",
    "b_my_model_concat_y = binary_my_model_concat_df[\"senti_label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am performing the train and test split for the google news dataset.\n",
    "b_g_news_concat_X_train, b_g_news_concat_X_test, b_g_news_concat_y_train, b_g_news_concat_y_test = train_test_split(b_g_news_concat_X, b_g_news_concat_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am performing the train and test split for the my model dataset.\n",
    "b_my_model_concat_X_train, b_my_model_concat_X_test, b_my_model_concat_y_train, b_my_model_concat_y_test = train_test_split(b_my_model_concat_X, b_my_model_concat_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am converting the training and testing input features and output features to tensors, by first converting them into a np.array(). Also for the output labels i am subtracting one from the class as the model need the labels to start from 0.\n",
    "b_g_news_X_train_tensor = torch.tensor(np.array(b_g_news_concat_X_train.tolist(), dtype=np.float32), dtype=torch.float32)\n",
    "b_g_news_y_train_tensor = torch.tensor(b_g_news_concat_y_train.apply(lambda x: x-1).to_numpy(), dtype=torch.long)\n",
    "b_g_news_X_test_tensor = torch.tensor(np.array(b_g_news_concat_X_test.tolist(),  dtype=np.float32) , dtype=torch.float32)\n",
    "b_g_news_y_test_tensor = torch.tensor(b_g_news_concat_y_test.apply(lambda x: x-1).to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the FNN classification model object and pushing it to the device. \n",
    "b_fnnModel = ClassiferModel(3000, 50, 10, 2).to(device)\n",
    "# Here i am using cross entropy loss as the loss function\n",
    "loss_function = nn.CrossEntropyLoss() \n",
    "# Here i am using the Adam optimizer with learning rate as 0.001\n",
    "optimizer = optim.Adam(b_fnnModel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the training dataset and data loader for batch processing.\n",
    "train_dataset = TensorDataset(b_g_news_X_train_tensor, b_g_news_y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.356680810970068\n",
      "Epoch 2/10, Loss: 0.30839536718428134\n",
      "Epoch 3/10, Loss: 0.2734541860073805\n",
      "Epoch 4/10, Loss: 0.23740782462358476\n",
      "Epoch 5/10, Loss: 0.20329186789505183\n",
      "Epoch 6/10, Loss: 0.1728428849829361\n",
      "Epoch 7/10, Loss: 0.14651444491287693\n",
      "Epoch 8/10, Loss: 0.12603916580602526\n",
      "Epoch 9/10, Loss: 0.10781032733311877\n",
      "Epoch 10/10, Loss: 0.09383634139711503\n"
     ]
    }
   ],
   "source": [
    "# Here i am training the model for 10 epoches with batch size of 32 \n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    b_fnnModel.train().to(device)  \n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "        optimizer.zero_grad()  \n",
    "        outputs = b_fnnModel(inputs) \n",
    "        loss = loss_function(outputs, labels) \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN Binary Classification Google News first 10 words Accuracy: 83.26%\n"
     ]
    }
   ],
   "source": [
    "# Here i am getting the test set accuracy of the model.\n",
    "b_fnnModel.eval()  \n",
    "with torch.no_grad():  \n",
    "    X_test_tensor, y_test_tensor = b_g_news_X_test_tensor.to(device), b_g_news_y_test_tensor.to(device) \n",
    "    outputs = b_fnnModel(X_test_tensor) \n",
    "    _, predicted = torch.max(outputs, 1)  \n",
    "    accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu())\n",
    "\n",
    "b_fnn_g_news_f10_acc = accuracy\n",
    "print(f'FNN Binary Classification Google News first 10 words Accuracy: {b_fnn_g_news_f10_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "del b_fnnModel\n",
    "for inputs, labels in train_loader:\n",
    "    del inputs\n",
    "    del labels\n",
    "del b_g_news_X_test_tensor\n",
    "del b_g_news_y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am converting the training and testing input features and output features to tensors, by first converting them into a np.array(). Also for the output labels i am subtracting one from the class as the model need the labels to start from 0.\n",
    "b_my_model_X_train_tensor = torch.tensor(np.array(b_my_model_concat_X_train.tolist() ,dtype=np.float32), dtype=torch.float32)\n",
    "b_my_model_y_train_tensor = torch.tensor(b_my_model_concat_y_train.apply(lambda x: x-1).to_numpy(), dtype=torch.long)\n",
    "b_my_model_X_test_tensor = torch.tensor(np.array(b_my_model_concat_X_test.tolist(), dtype=np.float32), dtype=torch.float32)\n",
    "b_my_model_y_test_tensor = torch.tensor(b_my_model_concat_y_test.apply(lambda x: x-1).to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the FNN classification model object and pushing it to the device. \n",
    "b_fnnModel = ClassiferModel(3000, 50, 10, 2).to(device)\n",
    "# Here i am using cross entropy loss as the loss function\n",
    "loss_function = nn.CrossEntropyLoss()  \n",
    "# Here i am using the Adam optimizer with learning rate as 0.001\n",
    "optimizer = optim.Adam(b_fnnModel.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the training dataset and data loader for batch processing.\n",
    "train_dataset = TensorDataset(b_my_model_X_train_tensor, b_my_model_y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.3249033022031188\n",
      "Epoch 2/10, Loss: 0.2890653444930911\n",
      "Epoch 3/10, Loss: 0.26353344066217543\n",
      "Epoch 4/10, Loss: 0.23594838480874897\n",
      "Epoch 5/10, Loss: 0.20920504307225346\n",
      "Epoch 6/10, Loss: 0.1859638928456232\n",
      "Epoch 7/10, Loss: 0.16456441350784154\n",
      "Epoch 8/10, Loss: 0.14609622634593397\n",
      "Epoch 9/10, Loss: 0.13207996216518805\n",
      "Epoch 10/10, Loss: 0.11790294027910568\n"
     ]
    }
   ],
   "source": [
    "# Here i am training the model for 10 epoches with batch size of 32 \n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    b_fnnModel.train() \n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device) \n",
    "        optimizer.zero_grad() \n",
    "        outputs = b_fnnModel(inputs) \n",
    "        loss = loss_function(outputs, labels) \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN Binary Classification My model first 10 words Accuracy: 85.02%\n"
     ]
    }
   ],
   "source": [
    "# Here i am getting the test set accuracy of the model.\n",
    "b_fnnModel.eval()  \n",
    "with torch.no_grad(): \n",
    "    X_test_tensor, y_test_tensor = b_my_model_X_test_tensor.to(device), b_my_model_y_test_tensor.to(device) \n",
    "    outputs = b_fnnModel(X_test_tensor)  \n",
    "    _, predicted = torch.max(outputs, 1)  \n",
    "    accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu())  \n",
    "\n",
    "b_fnn_my_model_f10_acc = accuracy\n",
    "print(f'FNN Binary Classification My model first 10 words Accuracy: {b_fnn_my_model_f10_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "del b_fnnModel\n",
    "for inputs, labels in train_loader:\n",
    "    del inputs\n",
    "    del labels\n",
    "del b_my_model_X_test_tensor\n",
    "del b_my_model_y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory deallocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "del binary_g_news_concat_df\n",
    "del binary_my_model_concat_df\n",
    "del b_g_news_concat_X\n",
    "del b_g_news_concat_y\n",
    "del b_my_model_concat_X\n",
    "del b_my_model_concat_y\n",
    "del b_g_news_concat_X_train\n",
    "del b_g_news_concat_X_test\n",
    "del b_g_news_concat_y_train\n",
    "del b_g_news_concat_y_test\n",
    "del b_my_model_concat_X_train\n",
    "del b_my_model_concat_X_test\n",
    "del b_my_model_concat_y_train\n",
    "del b_my_model_concat_y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am spliting the dataset into input label X and output label y for Ternary classification using google news embedding.\n",
    "t_g_news_concat_X = g_news_concat_df['review_embedding']\n",
    "t_g_news_concat_y = g_news_concat_df[\"senti_label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am spliting the dataset into input label X and output label y for Ternary classification using my model embedding.\n",
    "t_my_model_concat_X = my_model_concat_df['review_embedding']\n",
    "t_my_model_concat_y = my_model_concat_df[\"senti_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am performing the train and test split for the google news dataset\n",
    "t_g_news_concat_X_train, t_g_news_concat_X_test, t_g_news_concat_y_train, t_g_news_concat_y_test = train_test_split(t_g_news_concat_X, t_g_news_concat_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am performing the train and test split for the my model dataset\n",
    "t_my_model_concat_X_train, t_my_model_concat_X_test, t_my_model_concat_y_train, t_my_model_concat_y_test = train_test_split(t_my_model_concat_X, t_my_model_concat_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am converting the training and testing input features and output features to tensors, by first converting them into a np.array(). Also for the output labels i am subtracting one from the class as the model need the labels to start from 0.\n",
    "t_g_news_X_train_tensor = torch.tensor(np.array(t_g_news_concat_X_train.tolist(), dtype=np.float32), dtype=torch.float32)\n",
    "t_g_news_y_train_tensor = torch.tensor(t_g_news_concat_y_train.apply(lambda x: x-1).to_numpy(), dtype=torch.long)\n",
    "t_g_news_X_test_tensor = torch.tensor(np.array(t_g_news_concat_X_test.tolist(), dtype=np.float32), dtype=torch.float32)\n",
    "t_g_news_y_test_tensor = torch.tensor(t_g_news_concat_y_test.apply(lambda x: x-1).to_numpy(), dtype=torch.long) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the FNN classification model object and pushing it to the device. And, i am using cross entropy loss as the loss function. Also, i am using the Adam optimizer with learning rate as 0.001\n",
    "t_fnnModel = ClassiferModel(3000, 50, 10, 3).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(t_fnnModel.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the training dataset and data loader for batch processing.\n",
    "train_dataset = TensorDataset(t_g_news_X_train_tensor, t_g_news_y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6859153775024414\n",
      "Epoch 2/10, Loss: 0.6252996482491493\n",
      "Epoch 3/10, Loss: 0.588805819041729\n",
      "Epoch 4/10, Loss: 0.5519414755129814\n",
      "Epoch 5/10, Loss: 0.5177000136566162\n",
      "Epoch 6/10, Loss: 0.48498313717126845\n",
      "Epoch 7/10, Loss: 0.45538891998648645\n",
      "Epoch 8/10, Loss: 0.4293326005470753\n",
      "Epoch 9/10, Loss: 0.4053522188127041\n",
      "Epoch 10/10, Loss: 0.38391411766052247\n"
     ]
    }
   ],
   "source": [
    "# Here i am training the model for 10 epoches with batch size of 32 \n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    t_fnnModel.train()  \n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "        optimizer.zero_grad()  \n",
    "        outputs = t_fnnModel(inputs)  \n",
    "        loss = loss_function(outputs, labels)  \n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN Ternary Classification Google News first 10 words Accuracy: 68.42%\n"
     ]
    }
   ],
   "source": [
    "# Here i am getting the test set accuracy of the model.\n",
    "t_fnnModel.eval()  \n",
    "with torch.no_grad(): \n",
    "    X_test_tensor, y_test_tensor = t_g_news_X_test_tensor.to(device), t_g_news_y_test_tensor.to(device)\n",
    "    outputs = t_fnnModel(X_test_tensor) \n",
    "    _, predicted = torch.max(outputs, 1)  \n",
    "    accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu()) \n",
    "\n",
    "t_fnn_g_news_f10_acc = accuracy\n",
    "print(f'FNN Ternary Classification Google News first 10 words Accuracy: {t_fnn_g_news_f10_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "del t_fnnModel\n",
    "for inputs, labels in train_loader:\n",
    "    del inputs \n",
    "    del labels\n",
    "del t_g_news_X_test_tensor\n",
    "del t_g_news_y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am converting the training and testing input features and output features to tensors, by first converting them into a np.array(). Also for the output labels i am subtracting one from the class as the model need the labels to start from 0.\n",
    "t_my_model_X_train_tensor = torch.tensor(np.array(t_my_model_concat_X_train.tolist(), dtype=np.float32), dtype=torch.float32)\n",
    "t_my_model_y_train_tensor = torch.tensor(t_my_model_concat_y_train.apply(lambda x: x-1).to_numpy(), dtype=torch.long)\n",
    "t_my_model_X_test_tensor = torch.tensor(np.array(t_my_model_concat_X_test.tolist(), dtype=np.float32), dtype=torch.float32)\n",
    "t_my_model_y_test_tensor = torch.tensor(t_my_model_concat_y_test.apply(lambda x: x-1).to_numpy(), dtype=torch.long) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the FNN classification model object and pushing it to the device. And, i am using cross entropy loss as the loss function. Also, i am using the Adam optimizer with learning rate as 0.001\n",
    "t_fnnModel = ClassiferModel(3000, 50, 10, 3).to(device)\n",
    "loss_function = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(t_fnnModel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the training dataset and data loader for batch processing.\n",
    "train_dataset = TensorDataset(t_my_model_X_train_tensor, t_my_model_y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6482063727235794\n",
      "Epoch 2/10, Loss: 0.605265987098217\n",
      "Epoch 3/10, Loss: 0.577968397886753\n",
      "Epoch 4/10, Loss: 0.5511817797732353\n",
      "Epoch 5/10, Loss: 0.5252816880702973\n",
      "Epoch 6/10, Loss: 0.5023250477552414\n",
      "Epoch 7/10, Loss: 0.48019588971972466\n",
      "Epoch 8/10, Loss: 0.4602250667345524\n",
      "Epoch 9/10, Loss: 0.44289585554242134\n",
      "Epoch 10/10, Loss: 0.4272341419303417\n"
     ]
    }
   ],
   "source": [
    "# Here i am training the model for 10 epoches with batch size of 32 \n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    t_fnnModel.train() \n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = t_fnnModel(inputs) \n",
    "        loss = loss_function(outputs, labels) \n",
    "        loss.backward() \n",
    "        optimizer.step()  \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN Ternary Classification My Model first 10 words Accuracy: 70.94%\n"
     ]
    }
   ],
   "source": [
    "# Here i am getting the test set accuracy of the model.\n",
    "t_fnnModel.eval()  \n",
    "with torch.no_grad():  \n",
    "    X_test_tensor, y_test_tensor = t_my_model_X_test_tensor.to(device), t_my_model_y_test_tensor.to(device)  \n",
    "    outputs = t_fnnModel(X_test_tensor)  \n",
    "    _, predicted = torch.max(outputs, 1) \n",
    "    accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu()) \n",
    "\n",
    "t_fnn_my_model_f10_acc = accuracy\n",
    "print(f'FNN Ternary Classification My Model first 10 words Accuracy: {t_fnn_my_model_f10_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "del t_fnnModel\n",
    "for inputs, labels in train_loader:\n",
    "    del inputs \n",
    "    del labels\n",
    "del t_my_model_X_test_tensor\n",
    "del t_my_model_y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ternary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Deallocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "del g_news_concat_df\n",
    "del my_model_concat_df\n",
    "del t_g_news_concat_X\n",
    "del t_g_news_concat_y\n",
    "del t_my_model_concat_X\n",
    "del t_my_model_concat_y\n",
    "del t_g_news_concat_X_train\n",
    "del t_g_news_concat_X_test\n",
    "del t_g_news_concat_y_train\n",
    "del t_g_news_concat_y_test\n",
    "del t_my_model_concat_X_train\n",
    "del t_my_model_concat_X_test\n",
    "del t_my_model_concat_y_train\n",
    "del t_my_model_concat_y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am loading the google news and my model keyVectors to use for word embedding.\n",
    "wv = gensim.models.KeyedVectors.load(\"gensim_model_w2v_gnews_300.model\")\n",
    "my_model = gensim.models.Word2Vec.load(\"gensim_model_w2v_my_model.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy of the sampled df, in which the reviews are tokenized.\n",
    "sampled_cnn_df = sampled_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am limiting the size of the reviews to the first 50 tokens.\n",
    "sampled_cnn_df[\"review_body\"] = sampled_cnn_df[\"review_body\"].apply(lambda r: r[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the google news word embedding dataset.\n",
    "g_news_cnn_df = sampled_cnn_df.copy()\n",
    "g_news_cnn_df = g_news_cnn_df[[\"review_body\", \"senti_label\"]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the my model word embedding dataset.\n",
    "my_model_cnn_df = sampled_cnn_df.copy()\n",
    "my_model_cnn_df = my_model_cnn_df[[\"review_body\", \"senti_label\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sampled_cnn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am creating the CNN classifier class, with 2 layer of output size 50 and 10\n",
    "class CNNClassifierModel(nn.Module):\n",
    "    def __init__(self, embd_d, n_cls):\n",
    "        super(CNNClassifierModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=embd_d, out_channels=50, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=50, out_channels=10, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc = nn.Linear(10 * 50, n_cls)\n",
    "        \n",
    "    def forward(self, r):\n",
    "        r = r.permute(0, 2, 1)\n",
    "        \n",
    "        r = F.relu(self.conv1(r)) \n",
    "        r = F.relu(self.conv2(r)) \n",
    "        \n",
    "        r = r.view(r.size(0), -1)\n",
    "        r = self.fc(r)\n",
    "        \n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the Dataset Class to perform batch processing \n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, X, y, keyVec):\n",
    "        self.reviews = X\n",
    "        self.keyVec = keyVec\n",
    "        self.labels = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        label = self.labels[idx]\n",
    "        embedding = np.zeros((50, 300), dtype=np.float32)\n",
    "        \n",
    "        # Here i am creating the [50, 300] sentence embedding matrix for each of the review.\n",
    "        for ind in range(50):\n",
    "            if ind < len(review) and review[ind] in self.keyVec:\n",
    "                embedding[ind] = self.keyVec[review[ind]]\n",
    "            else:\n",
    "                embedding[ind] = np.zeros(300, dtype=np.float32)\n",
    "        \n",
    "        return torch.tensor(embedding, dtype=torch.float32), torch.tensor(label, dtype=torch.long) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs the sentence embedding and create a [50, 300] matrix as a result.\n",
    "def review_50_embedding(review, keyVec):\n",
    "    w_vec = np.zeros((50, 300), dtype=np.float32)\n",
    "    for ind in range(50):\n",
    "        if ind < len(review) and review[ind] in keyVec:\n",
    "            w_vec[ind] = keyVec[review[ind]]\n",
    "        else:\n",
    "            w_vec[ind] = np.zeros(300)\n",
    "    return w_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data for binary classification with google news embedding.\n",
    "binary_g_news_cnn_df = g_news_cnn_df.copy()\n",
    "binary_g_news_cnn_df = binary_g_news_cnn_df[binary_g_news_cnn_df[\"senti_label\"] != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data for binary classification with my model embedding.\n",
    "binary_my_model_cnn_df = my_model_cnn_df.copy()\n",
    "binary_my_model_cnn_df = binary_my_model_cnn_df[binary_my_model_cnn_df[\"senti_label\"] != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the input and output features for the binary classification with google news embedding\n",
    "b_g_news_cnn_X = binary_g_news_cnn_df['review_body']\n",
    "b_g_news_cnn_y = binary_g_news_cnn_df[\"senti_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the input and output features for the binary classification with my model embedding\n",
    "b_my_model_cnn_X = binary_my_model_cnn_df['review_body']\n",
    "b_my_model_cnn_y = binary_my_model_cnn_df[\"senti_label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am performing the train and test split for google news embeddings\n",
    "b_g_news_cnn_X_train, b_g_news_cnn_X_test, b_g_news_cnn_y_train, b_g_news_cnn_y_test = train_test_split(b_g_news_cnn_X, b_g_news_cnn_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am performing the train and test split for my model embeddings\n",
    "b_my_model_cnn_X_train, b_my_model_cnn_X_test, b_my_model_cnn_y_train, b_my_model_cnn_y_test = train_test_split(b_my_model_cnn_X, b_my_model_cnn_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the test input and output features into tensors. For the output label i am subtracing 1, to make them start with 0.\n",
    "b_g_news_X_test_tensor = torch.tensor(np.array([review_50_embedding(r, wv) for r in b_g_news_cnn_X_test], dtype=np.float32), dtype=torch.float32)\n",
    "b_g_news_y_test_tensor = torch.tensor(b_g_news_cnn_y_test.apply(lambda x: x-1).to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the CNN classifier object and using the cross entropy loss function and adam optimizer.\n",
    "b_cnnModel = CNNClassifierModel(300, 2).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(b_cnnModel.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am reseting the indexing for the training input and output features.\n",
    "b_g_news_cnn_X_train = b_g_news_cnn_X_train.reset_index(drop=True)  \n",
    "b_g_news_cnn_y_train = b_g_news_cnn_y_train.reset_index(drop=True)\n",
    "\n",
    "# Here i am subtracting 1 from the training output feature to make the label start from 0.\n",
    "b_g_news_cnn_y_train = b_g_news_cnn_y_train.apply(lambda x: x-1)\n",
    "\n",
    "# Creating the training dataset and dataloader for batch processing.\n",
    "train_dataset = ReviewDataset(b_g_news_cnn_X_train, b_g_news_cnn_y_train, wv)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.2760045650832355\n",
      "Epoch 2/10, Loss: 0.2232752208404243\n",
      "Epoch 3/10, Loss: 0.20167210635878147\n",
      "Epoch 4/10, Loss: 0.1836438637746498\n",
      "Epoch 5/10, Loss: 0.16774623184874654\n",
      "Epoch 6/10, Loss: 0.15336882965639234\n",
      "Epoch 7/10, Loss: 0.14054114817213267\n",
      "Epoch 8/10, Loss: 0.1290992124103941\n",
      "Epoch 9/10, Loss: 0.11856321447109804\n",
      "Epoch 10/10, Loss: 0.10961677876082249\n"
     ]
    }
   ],
   "source": [
    "# Here i am training the model for 10 epoches and batch size of 32.\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    b_cnnModel.train().to(device)\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "        optimizer.zero_grad() \n",
    "        outputs = b_cnnModel(inputs) \n",
    "        loss = loss_function(outputs, labels)  \n",
    "        loss.backward() \n",
    "        optimizer.step()  \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Binary Classification Google News Accuracy: 89.89%\n"
     ]
    }
   ],
   "source": [
    "# Here i am getting the test accuracy of the model.\n",
    "b_cnnModel.eval()  \n",
    "with torch.no_grad(): \n",
    "    X_test_tensor, y_test_tensor = b_g_news_X_test_tensor.to(device), b_g_news_y_test_tensor.to(device) \n",
    "    outputs = b_cnnModel(X_test_tensor)  \n",
    "    _, predicted = torch.max(outputs, 1) \n",
    "    accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu())\n",
    "\n",
    "b_cnn_g_news_acc = accuracy\n",
    "print(f'CNN Binary Classification Google News Accuracy: {b_cnn_g_news_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "del b_cnnModel\n",
    "for inputs, labels in train_loader:\n",
    "    del inputs\n",
    "    del labels\n",
    "del b_g_news_X_test_tensor\n",
    "del b_g_news_y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the test input and output features into tensors. For the output label i am subtracing 1, to make them start with 0.\n",
    "b_my_model_X_test_tensor = torch.tensor(np.array([review_50_embedding(r, my_model) for r in b_my_model_cnn_X_test], dtype=np.float32), dtype=torch.float32)\n",
    "b_my_model_y_test_tensor = torch.tensor(b_my_model_cnn_y_test.apply(lambda x: x-1).to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am reseting the indexing for the training input and output features.\n",
    "b_my_model_cnn_X_train = b_my_model_cnn_X_train.reset_index(drop=True)  \n",
    "b_my_model_cnn_y_train = b_my_model_cnn_y_train.reset_index(drop=True)  \n",
    "\n",
    "# Here i am subtracting 1 from the training output feature to make the label start from 0.\n",
    "b_my_model_cnn_y_train = b_my_model_cnn_y_train.apply(lambda x: x-1)\n",
    "\n",
    "# Creating the training dataset and dataloader for batch processing.\n",
    "train_dataset = ReviewDataset(b_my_model_cnn_X_train, b_my_model_cnn_y_train, my_model)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the CNN classifier object and using the cross entropy loss function and adam optimizer.\n",
    "b_cnnModel = CNNClassifierModel(300, 2).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(b_cnnModel.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.26135384093225\n",
      "Epoch 2/10, Loss: 0.23069126994162797\n",
      "Epoch 3/10, Loss: 0.21776947853192688\n",
      "Epoch 4/10, Loss: 0.20774835375770925\n",
      "Epoch 5/10, Loss: 0.1985382195169106\n",
      "Epoch 6/10, Loss: 0.19087891874536872\n",
      "Epoch 7/10, Loss: 0.18231498567909002\n",
      "Epoch 8/10, Loss: 0.1754128668885678\n",
      "Epoch 9/10, Loss: 0.16834453330524266\n",
      "Epoch 10/10, Loss: 0.16105076786614955\n"
     ]
    }
   ],
   "source": [
    "# Here i am training the model for 10 epoches and batch size of 32.\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    b_cnnModel.train().to(device)  \n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "        optimizer.zero_grad()  \n",
    "        outputs = b_cnnModel(inputs)  \n",
    "        loss = loss_function(outputs, labels)  \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Binary Classification My Model Accuracy: 90.40%\n"
     ]
    }
   ],
   "source": [
    "# Here i am getting the test accuracy of the model\n",
    "b_cnnModel.eval() \n",
    "with torch.no_grad(): \n",
    "    X_test_tensor, y_test_tensor = b_my_model_X_test_tensor.to(device), b_my_model_y_test_tensor.to(device)  \n",
    "    outputs = b_cnnModel(X_test_tensor)  \n",
    "    _, predicted = torch.max(outputs, 1)  \n",
    "    accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu())\n",
    "\n",
    "b_cnn_my_model_acc = accuracy\n",
    "print(f'CNN Binary Classification My Model Accuracy: {b_cnn_my_model_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "del b_cnnModel\n",
    "for inputs, labels in train_loader:\n",
    "    del inputs\n",
    "    del labels\n",
    "del b_my_model_X_test_tensor\n",
    "del b_my_model_y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Deallocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "del binary_g_news_cnn_df\n",
    "del binary_my_model_cnn_df\n",
    "del b_g_news_cnn_X\n",
    "del b_g_news_cnn_y\n",
    "del b_my_model_cnn_X\n",
    "del b_my_model_cnn_y\n",
    "del b_g_news_cnn_X_train\n",
    "del b_g_news_cnn_X_test\n",
    "del b_g_news_cnn_y_train\n",
    "del b_g_news_cnn_y_test\n",
    "del b_my_model_cnn_X_train\n",
    "del b_my_model_cnn_X_test\n",
    "del b_my_model_cnn_y_train\n",
    "del b_my_model_cnn_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ternary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the ternary classificationd dataset for google news.\n",
    "t_g_news_cnn_X = g_news_cnn_df['review_body']\n",
    "t_g_news_cnn_y = g_news_cnn_df[\"senti_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the ternary classificationd dataset for my model.\n",
    "t_my_model_cnn_X = my_model_cnn_df['review_body']\n",
    "t_my_model_cnn_y = my_model_cnn_df[\"senti_label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am performing train and test split for google news\n",
    "t_g_news_cnn_X_train, t_g_news_cnn_X_test, t_g_news_cnn_y_train, t_g_news_cnn_y_test = train_test_split(t_g_news_cnn_X, t_g_news_cnn_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am performing train and test split for my model\n",
    "t_my_model_cnn_X_train, t_my_model_cnn_X_test, t_my_model_cnn_y_train, t_my_model_cnn_y_test = train_test_split(t_my_model_cnn_X, t_my_model_cnn_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the test input and output features into tensors. For the output label i am subtracing 1, to make them start with 0.\n",
    "t_g_news_X_test_tensor = torch.tensor(np.array([review_50_embedding(r, wv) for r in t_g_news_cnn_X_test], dtype=np.float32), dtype=torch.float32)\n",
    "t_g_news_y_test_tensor = torch.tensor(t_g_news_cnn_y_test.apply(lambda x: x-1).to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the CNN classifier object and using the cross entropy loss function and adam optimizer.\n",
    "t_cnnModel = CNNClassifierModel(300, 3).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(t_cnnModel.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am reseting the indexing for the training input and output features.\n",
    "t_g_news_cnn_X_train = t_g_news_cnn_X_train.reset_index(drop=True)  \n",
    "t_g_news_cnn_y_train = t_g_news_cnn_y_train.reset_index(drop=True)\n",
    "\n",
    "# Here i am subtracting 1 from the training output feature to make the label start from 0.\n",
    "t_g_news_cnn_y_train = t_g_news_cnn_y_train.apply(lambda x: x-1)\n",
    "\n",
    "# Creating the training dataset and dataloader for batch processing.\n",
    "train_dataset = ReviewDataset(t_g_news_cnn_X_train, t_g_news_cnn_y_train, wv)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6003190448737145\n",
      "Epoch 2/10, Loss: 0.5331146879816056\n",
      "Epoch 3/10, Loss: 0.5106789461946487\n",
      "Epoch 4/10, Loss: 0.49393784275650976\n",
      "Epoch 5/10, Loss: 0.479999182240963\n",
      "Epoch 6/10, Loss: 0.46754138472795487\n",
      "Epoch 7/10, Loss: 0.45617883866548536\n",
      "Epoch 8/10, Loss: 0.4458430671775341\n",
      "Epoch 9/10, Loss: 0.43673794154882434\n",
      "Epoch 10/10, Loss: 0.4290642709851265\n"
     ]
    }
   ],
   "source": [
    "# Here i am training the model.\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    t_cnnModel.train() \n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device) \n",
    "        optimizer.zero_grad() \n",
    "        outputs = t_cnnModel(inputs)  \n",
    "        loss = loss_function(outputs, labels) \n",
    "        loss.backward()  \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Ternary Classification Google News Accuracy: 76.48%\n"
     ]
    }
   ],
   "source": [
    "# Here i am getting the test accuracy of the model\n",
    "t_cnnModel.eval() \n",
    "with torch.no_grad(): \n",
    "    X_test_tensor, y_test_tensor = t_g_news_X_test_tensor.to(device), t_g_news_y_test_tensor.to(device)\n",
    "    outputs = t_cnnModel(X_test_tensor) \n",
    "    _, predicted = torch.max(outputs, 1)  \n",
    "    accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu())\n",
    "\n",
    "t_cnn_g_news_acc = accuracy\n",
    "print(f'CNN Ternary Classification Google News Accuracy: {t_cnn_g_news_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "del t_cnnModel\n",
    "for inputs, labels in train_loader:\n",
    "    del inputs \n",
    "    del labels\n",
    "del t_g_news_X_test_tensor\n",
    "del t_g_news_y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the test input and output features into tensors. For the output label i am subtracing 1, to make them start with 0.\n",
    "t_my_model_X_test_tensor = torch.tensor(np.array([review_50_embedding(r, my_model) for r in t_my_model_cnn_X_test], dtype=np.float32), dtype=torch.float32)\n",
    "t_my_model_y_test_tensor = torch.tensor(t_my_model_cnn_y_test.apply(lambda x: x-1).to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am creating the CNN classifier object and using the cross entropy loss function and adam optimizer.\n",
    "t_cnnModel = CNNClassifierModel(300, 3).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(t_cnnModel.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i am reseting the indexing for the training input and output features.\n",
    "t_my_model_cnn_X_train = t_my_model_cnn_X_train.reset_index(drop=True)  \n",
    "t_my_model_cnn_y_train = t_my_model_cnn_y_train.reset_index(drop=True)  \n",
    "\n",
    "# Here i am subtracting 1 from the training output feature to make the label start from 0.\n",
    "t_my_model_cnn_y_train = t_my_model_cnn_y_train.apply(lambda x: x-1)\n",
    "\n",
    "# Creating the training dataset and dataloader for batch processing.\n",
    "train_dataset = ReviewDataset(t_my_model_cnn_X_train, t_my_model_cnn_y_train, my_model)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5825681155824661\n",
      "Epoch 2/10, Loss: 0.5468351379394532\n",
      "Epoch 3/10, Loss: 0.5334672194004059\n",
      "Epoch 4/10, Loss: 0.5234786265206337\n",
      "Epoch 5/10, Loss: 0.5155383927822113\n",
      "Epoch 6/10, Loss: 0.5081573192715645\n",
      "Epoch 7/10, Loss: 0.5013403821313381\n",
      "Epoch 8/10, Loss: 0.49499311629772186\n",
      "Epoch 9/10, Loss: 0.4889717603087425\n",
      "Epoch 10/10, Loss: 0.4842364546084404\n"
     ]
    }
   ],
   "source": [
    "# Here i am training the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    t_cnnModel.train()  \n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device) \n",
    "        optimizer.zero_grad()  \n",
    "        outputs = t_cnnModel(inputs) \n",
    "        loss = loss_function(outputs, labels)  \n",
    "        loss.backward() \n",
    "        optimizer.step()  \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Ternary Classification My Model Accuracy: 76.80%\n"
     ]
    }
   ],
   "source": [
    "# Here i am getting the test accuracy of the model\n",
    "t_cnnModel.eval()  \n",
    "with torch.no_grad(): \n",
    "    X_test_tensor, y_test_tensor = t_my_model_X_test_tensor.to(device), t_my_model_y_test_tensor.to(device)  \n",
    "    outputs = t_cnnModel(X_test_tensor)  \n",
    "    _, predicted = torch.max(outputs, 1)  \n",
    "    accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu())\n",
    "\n",
    "t_cnn_my_model_acc = accuracy\n",
    "print(f'CNN Ternary Classification My Model Accuracy: {t_cnn_my_model_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "del t_cnnModel\n",
    "for inputs, labels in train_loader:\n",
    "    del inputs \n",
    "    del labels\n",
    "del t_my_model_X_test_tensor\n",
    "del t_my_model_y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Accuracy Values for all the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron\n",
      "------------------------------------------------------------\n",
      "The Binary Classification (Google News 300): 0.822\n",
      "The Binary Classification (My Model): 0.833025\n",
      "\n",
      "\n",
      "SVM\n",
      "------------------------------------------------------------\n",
      "The Binary Classification (Google News 300): 0.845425\n",
      "The Binary Classification (My Model): 0.887025\n",
      "\n",
      "\n",
      "FNN\n",
      "------------------------------------------------------------\n",
      "The Binary Classification Average (Google News 300): 0.8774\n",
      "The Binary Classification Average (My Model): 0.898975\n",
      "\n",
      "The Ternary Classification Average (Google News 300): 0.73546\n",
      "The Ternary Classification Average (My Model): 0.76388\n",
      "\n",
      "The Binary Classification First 10 (Google News 300): 0.832625\n",
      "The Binary Classification First 10 (My Model): 0.850175\n",
      "\n",
      "The Ternary Classification First 10 (Google News 300): 0.6842\n",
      "The Ternary Classification First 10 (My Model): 0.70944\n",
      "\n",
      "\n",
      "CNN\n",
      "------------------------------------------------------------\n",
      "The Binary Classification (Google News 300): 0.89885\n",
      "The Binary Classification (My Model): 0.904\n",
      "\n",
      "The Ternary Classification (Google News 300): 0.76476\n",
      "The Ternary Classification (My Model): 0.76804\n"
     ]
    }
   ],
   "source": [
    "print(\"Perceptron\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f'The Binary Classification (Google News 300): {perceptron_g_news_acc}')\n",
    "print(f'The Binary Classification (My Model): {perceptron_my_model_acc}')\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"SVM\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f'The Binary Classification (Google News 300): {svm_g_news_acc}')\n",
    "print(f'The Binary Classification (My Model): {svm_my_model_acc}')\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"FNN\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f'The Binary Classification Average (Google News 300): {b_fnn_g_news_avg_acc}')\n",
    "print(f'The Binary Classification Average (My Model): {b_fnn_my_model_avg_acc}')\n",
    "print(\"\")\n",
    "print(f'The Ternary Classification Average (Google News 300): {t_fnn_g_news_avg_acc}')\n",
    "print(f'The Ternary Classification Average (My Model): {t_fnn_my_model_avg_acc}')\n",
    "print(\"\")\n",
    "print(f'The Binary Classification First 10 (Google News 300): {b_fnn_g_news_f10_acc}')\n",
    "print(f'The Binary Classification First 10 (My Model): {b_fnn_my_model_f10_acc}')\n",
    "print(\"\")\n",
    "print(f'The Ternary Classification First 10 (Google News 300): {t_fnn_g_news_f10_acc}')\n",
    "print(f'The Ternary Classification First 10 (My Model): {t_fnn_my_model_f10_acc}')\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"CNN\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f'The Binary Classification (Google News 300): {b_cnn_g_news_acc}')\n",
    "print(f'The Binary Classification (My Model): {b_cnn_my_model_acc}')\n",
    "print(\"\")\n",
    "print(f'The Ternary Classification (Google News 300): {t_cnn_g_news_acc}')\n",
    "print(f'The Ternary Classification (My Model): {t_cnn_my_model_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
