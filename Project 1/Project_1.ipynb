{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Project 1</h1></center>\n",
    "<br>\n",
    "<center><font size=\"5\">Name - Spandan Patil</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Prepration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have read the amazon kitchen products review dataset and stored it into a pandas Dataframe. I have concatinated the review_headline column into the review_body column, thus, using the review_body column as my input feature. Also, I am using the star_ratings column as my output label. When I the statistics of the star_ratings columns i found that the datatype of the values is not uniform, there are some values present in string type and some invalid date values are also present. Hence, I have applied few preprocessing and data cleaning step to star rating column to make it uniform. The step are as follows: \n",
    "1) Converting all the values present in the column to numeric, and if we cannot convert them to numeric then set them as NaN. \n",
    "2) Dropping all the records having the NaN value, these would be mainly the invalid date values in our case.\n",
    "\n",
    "**The 3 Sample reviews are as follows:**\n",
    "\n",
    "| Reviews    | Ratings      | \n",
    "|----------------|---------------|\n",
    "| Five Stars Great product.  | 5 | \n",
    "| Phffffffft, Phfffffft. Lots of air, and it's Cool! What's to say about this commodity item except, what have we come to in this world.<br />Having the need to bnuy captured and compressed air. &#60;lol&#62; | 5 | \n",
    "| but I am sure I will like it. Haven't used yet, but I am sure I will like it.| 5 |\n",
    "\n",
    "**The Statistics of the Rating column after some cleaning are as follows:**\n",
    "\n",
    "|star_rating| Counts|\n",
    "|-----------|--------|\n",
    "| 5.0  |  1582812 |\n",
    "| 4.0   |  418371 |\n",
    "| 1.0    | 306979 |\n",
    "| 3.0    | 193691 |\n",
    "| 2.0    | 138384 |\n",
    "\n",
    "**The Statistics of the reviews in each of the 3 Classes are as follows:**\n",
    "\n",
    "| Positive | Neutral | Negative |\n",
    "|------------|------------|-------|\n",
    "| 2001183 | 193691 | 445363 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For performing the data cleaning steps, i have created a clean_review function which i have applied to each value present in review_body column. Inside the clean_review function, i am doing the following steps in order:\n",
    "\n",
    "1) Converting the review into string object and then converting it to lowercase using in-built python functions\n",
    "2) I am removing the HTML tags and elements by using the Beautiful Soup HTML parser to convert the review into an beautiful soup object and extracting only the text data from it.\n",
    "3) To remove the URLS, I am using an regex to detect any words starting with http or www and replacing the whole word with blank space.\n",
    "4) For converting the contradictions to their expanded form, I have created a contradiction dictionary where the contradictions are the keys and their expanded forms are the values. I am iterating each key in the contradiction dictionary and checking if that key exists in the review then replace it with its expanded form.\n",
    "5) Using regex i am replacing all the non-alphabetic characters except for the spaces with a blank space.\n",
    "6) Again using regex i am replacing any consecutive space with a single space.\n",
    "7) Before returning the review i am removing any leading or trailing spaces using the strip function.\n",
    "\n",
    "**The statistics for average length of reviews are as follows:**\n",
    "\n",
    "|The average length of reviews before cleaning| The average length of reviews after cleaning|\n",
    "|-----------|--------|\n",
    "| 342.90 |  326.14 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For removing the stopwords i have created a simple function remove_stopwords which i am applying to each value of the review_body column. In the remove_stopwords function, I am converting the review string into a list with single space as separator. Hence, i am iterating over the review list and discarding the word if it is present in the nltk english stopwords set. At the end, I am joining the filtered words with a single space, thus converting them into a string and returning that review.\n",
    "\n",
    "For performing lemmatization, I have used nltk WordNetLemmatizer and using it create function perfom_lemmatization. In the function, I am converting the review string into a list with single space as separator. Hence, i am iterating over the review list and converting each word into its root form using the WordNetLemmatizer, i have assumed each word to be verb by setting the pos tag to 'v'. At the end, I am joining the root words with a single space, thus converting them into a string and returning that review.\n",
    "\n",
    "**The 3 Sample reviews before data cleaning and preprocessing are as follows:**\n",
    "\n",
    "| Reviews    | Ratings      | \n",
    "|----------------|---------------|\n",
    "| Four Stars These are perfect for &#34;the guys&#34; to enjoy while the ladies are attending the baby shower!  | 1 | \n",
    "| Five Stars Works great for late night private reading without disturbing others. | 1 | \n",
    "| great price an no problems price was very good. I haven't had any problems.| 1 |\n",
    "\n",
    "\n",
    "**The 3 Sample reviews after data cleaning and preprocessing are as follows:**\n",
    "\n",
    "| Reviews    | Ratings      | \n",
    "|----------------|---------------|\n",
    "| four star perfect guy enjoy ladies attend baby shower  | 1 | \n",
    "| five star work great late night private read without disturb others | 1 | \n",
    "| great price problems price good problems| 1 |\n",
    "\n",
    "**The statistics for average length of reviews are as follows:**\n",
    "\n",
    "|The average length of reviews before data cleaning and preprocessing| The average length of reviews after data cleaning and preprocessing|\n",
    "|-----------|--------|\n",
    "| 326.14 |  199.40 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step i am using TfidfVectorizer from the sklearn library to convert the value present in review_body column into TF-IDF Feature vector. Since it is a sparse vector it will result in forming a sparse matrix of TF-IDF features for each value present in review_body column. Then using the pandas in-built function i am converting this sparse matrix back into a pandas data frame. So that i can concatenate it with the output label sent_label and perform the train test split on the resultant dataframe. I am using all the features which are extracted by the TfidfVectorizer as my input features which are a total of 126785 features (aa to zzzzzzz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am using percepton model with hyperparamter tuning of increasing the number of epoch to 10000 and enabling early stopping if the accuracy doesn't increase.\n",
    "\n",
    "**The performance statistics of the model are as follows:**\n",
    "\n",
    "|     | Accuracy | Precision | Recall | F1 Score |\n",
    "|------| ---------|----------|---------|----------|\n",
    "| Train | 0.9363 | 0.9427 | 0.9292 | 0.9359 |\n",
    "| Test | 0.8923 | 0.9008 | 0.8803| 0.8904 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am using LinearSVC model with hyperparamter tuning of increasing the number of epoch to 10000. I am using LinearSVC here instead of SVC due to the large size of data. In the Sklearn documentation, it is given that using SVC may be impratical beyond the tens of thousands of samples, and LinearSVC is preferred for larger datasets.\n",
    "\n",
    "**The performance statistics of the model are as follows:**\n",
    "\n",
    "|     | Accuracy | Precision | Recall | F1 Score |\n",
    "|------| ---------|----------|---------|----------|\n",
    "| Train | 0.9566 | 0.9581 | 0.9550 | 0.9566 |\n",
    "| Test | 0.9204 | 0.9213 | 0.9183| 0.9198 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am using Logistic Rgression model with hyperparamter tuning of increasing the number of epoch to 10000 and using the solver algorithm as liblinear, since it is giving very slightly better test accuracy than default lbfgs.\n",
    "\n",
    "**The performance statistics of the model are as follows:**\n",
    "\n",
    "|     | Accuracy | Precision | Recall | F1 Score |\n",
    "|------| ---------|----------|---------|----------|\n",
    "| Train | 0.9324 | 0.9360 | 0.9286 | 0.9323 |\n",
    "| Test | 0.9228 | 0.9252 | 0.9191 | 0.9222 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i am using the default Multinomial Naive Bayes model without any hyperparameter tuning.\n",
    "\n",
    "**The performance statistics of the model are as follows:**\n",
    "\n",
    "|     | Accuracy | Precision | Recall | F1 Score |\n",
    "|------| ---------|----------|---------|----------|\n",
    "| Train | 0.8934 | 0.9168 | 0.8657 | 0.8905 |\n",
    "| Test | 0.8765 | 0.8975 | 0.8484 | 0.8723 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Spandan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am reading the amazon kitchen products review dataset and storing it into a Dataframe object using Pandas\n",
    "\n",
    "df = pd.read_csv('./amazon_reviews_us_Office_Products_v1_00.tsv', sep='\\t', on_bad_lines=\"skip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2640254 entries, 0 to 2640253\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   marketplace        object \n",
      " 1   customer_id        int64  \n",
      " 2   review_id          object \n",
      " 3   product_id         object \n",
      " 4   product_parent     int64  \n",
      " 5   product_title      object \n",
      " 6   product_category   object \n",
      " 7   star_rating        object \n",
      " 8   helpful_votes      float64\n",
      " 9   total_votes        float64\n",
      " 10  vine               object \n",
      " 11  verified_purchase  object \n",
      " 12  review_headline    object \n",
      " 13  review_body        object \n",
      " 14  review_date        object \n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 302.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2640254 entries, 0 to 2640253\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Dtype \n",
      "---  ------       ----- \n",
      " 0   review_body  object\n",
      " 1   star_rating  object\n",
      "dtypes: object(2)\n",
      "memory usage: 40.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df[\"review_body\"] = df[\"review_headline\"] + \" \" + df[\"review_body\"]\n",
    "df = df[[\"review_body\", \"star_rating\"]]\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Sample reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                      review_body  \\\n",
      "0                                                                                                                                                                                       Five Stars Great product.   \n",
      "1  Phffffffft, Phfffffft. Lots of air, and it's Cool! What's to say about this commodity item except, what have we come to in this world.<br />Having the need to bnuy captured and compressed air. &#60;lol&#62;   \n",
      "2                                                                                                                                   but I am sure I will like it. Haven't used yet, but I am sure I will like it.   \n",
      "\n",
      "  star_rating  \n",
      "0           5  \n",
      "1           5  \n",
      "2           5  \n"
     ]
    }
   ],
   "source": [
    "# Below are the 3 Sample reviews and their corresponding rating \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistic of Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "star_rating\n",
      "5             1459036\n",
      "4              389612\n",
      "1              286080\n",
      "3              179871\n",
      "2              129033\n",
      "5              123776\n",
      "4               28759\n",
      "1               20899\n",
      "3               13820\n",
      "2                9351\n",
      "2015-06-05          1\n",
      "2015-02-11          1\n",
      "2014-02-14          1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Below are the statistics of the star_rating column, here we can see that all the values present in this column are not uniform, some are of type int and some are of type string. Also, some invalid values like date values are present. We need to remove these invalid date columns by dropping the rows and converts all the string values into int type to get a uniform type for the star_rating column.\n",
    "\n",
    "print(df[\"star_rating\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We form three classes and select 20000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "star_rating\n",
      "5.0    1582812\n",
      "4.0     418371\n",
      "1.0     306979\n",
      "3.0     193691\n",
      "2.0     138384\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Here I am converting all the values present in the star_rating columns into numeric using the in-built pandas function to_numeric, also by setting the errors parameter to 'coerce' all the values which are not able to be converted to numeric will be set to NaN. In our case these would be mainly the rows having the invalid date values. Hence, we can just drop all the NaN value rows present in the star_ratings column which are be generated using the dropna function our pandas dataframe. After the processing we can see that all the rows present in the dataset are having a star_ratings label from 1 to 5.\n",
    "\n",
    "df['star_rating'] = pd.to_numeric(df['star_rating'], errors='coerce')\n",
    "df = df.dropna(subset=['star_rating'])\n",
    "print(df[\"star_rating\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive reviews are: 2001183\n",
      "The number of neutral reviews are: 193691\n",
      "The number of negative reviews are: 445363\n"
     ]
    }
   ],
   "source": [
    "# Here I am creating a copy of the original df on which we will perform are modification and convert the star_ratings column to senti_label column which will have 1/0 values representing the positive and negative sentiments.\n",
    "new_df = df.copy()\n",
    "\n",
    "# Using the origianl Dataframe i am counting all the rows having neutral reviews\n",
    "count_of_neutral_reviews = len(df[df[\"star_rating\"] == 3])\n",
    "\n",
    "# In the new Dataframe i am keeping only the rows which are having a rating above 3 or equal to or below 2, i.e positive and negative reviews and discarding the neutral reivews having star rating of 3.\n",
    "new_df = new_df[new_df[\"star_rating\"] != 3]\n",
    "\n",
    "# Here I am applying a simple lambda function to check if the rating is above 3 then the senti_label will have value 1 (positive) or else if the rating is 2 or below it will have value to 0 (negative).\n",
    "new_df[\"senti_label\"] = new_df[\"star_rating\"].apply(lambda r: 1 if r > 3 else 0)\n",
    "\n",
    "# Here I am keeping only the review_body as our input feature and senti_label as our processed output label and discarding the star_ratings column now.\n",
    "new_df = new_df[[\"review_body\", \"senti_label\"]]\n",
    "\n",
    "# Here i am simply counting the number of rows having positive and negative sentiments in the senti_label column.\n",
    "count_of_positive_reviews = len(new_df[new_df[\"senti_label\"] == 1])\n",
    "count_of_negative_reviews = len(new_df[new_df[\"senti_label\"] == 0])\n",
    "\n",
    "print(f'The number of positive reviews are: {count_of_positive_reviews}')\n",
    "print(f'The number of neutral reviews are: {count_of_neutral_reviews}')\n",
    "print(f'The number of negative reviews are: {count_of_negative_reviews}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here for all the random sampling i am set a random rate of 34, to acheieve repeatable result for the ease of showing examples and explaination.\n",
    "r_state = 34\n",
    "\n",
    "# Here I am randomly sampling 100000 rows of both positive and negative reviews. Concating them together and again shuffling the whole column and finally adding it back to the dataframe. Also new index values are assigned to the columns after shuffling to keep the readability.\n",
    "positive_class = new_df[new_df[\"senti_label\"] == 1]\n",
    "negative_class = new_df[new_df[\"senti_label\"] == 0]\n",
    "\n",
    "positive_sample = positive_class.sample(n=100000, random_state=r_state)\n",
    "negative_sample = negative_class.sample(n=100000, random_state=r_state)\n",
    "\n",
    "sampled_df = pd.concat([positive_sample, negative_sample])\n",
    "\n",
    "sampled_df = sampled_df.sample(frac=1, random_state=r_state).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senti_label\n",
      "1    100000\n",
      "0    100000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Here as you can see we have 100000 record of each positive and negative reviews in our dataframe.\n",
    "counts = sampled_df[\"senti_label\"].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                    review_body  \\\n",
      "0  Four Stars These are perfect for &#34;the guys&#34; to enjoy while the ladies are attending the baby shower!   \n",
      "1                              Five Stars Works great for late night private reading without disturbing others.   \n",
      "2                                   great price an no problems price was very good. I haven't had any problems.   \n",
      "\n",
      "   senti_label  \n",
      "0            1  \n",
      "1            1  \n",
      "2            1  \n"
     ]
    }
   ],
   "source": [
    "# Here are 3 Sample reviews after the processing, sampling and shuffling of the postive and negative reviews.\n",
    "\n",
    "sample_reviews_before_cleaning = sampled_df.head(3)\n",
    "print(sample_reviews_before_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this function we are converting the reviews into lowercase, removing all the html and urls, removing unnecessary non-alphabetical Characters, removing extra spaces and performing the contradictions.\n",
    "def clean_review(review):\n",
    "\n",
    "    # This is the dictionary having all the commonly used contradictions in the lower case format. These include format with ' or without it. For example didn't and didnt both are included.\n",
    "    contractions_dict = {\n",
    "        \"i'm\": \"i am\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"you've\": \"you have\",\n",
    "        \"he's\": \"he has\",\n",
    "        \"she's\": \"she has\",\n",
    "        \"it's\": \"it has\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"here's\": \"here is\",\n",
    "        \"didnt\": \"did not\",\n",
    "        \"cant\": \"cannot\",\n",
    "        \"wont\": \"will not\",\n",
    "        \"dont\": \"do not\",\n",
    "        \"doesnt\": \"does not\",\n",
    "        \"shouldnt\": \"should not\",\n",
    "        \"wouldnt\": \"would not\",\n",
    "        \"mustnt\": \"must not\",\n",
    "        \"neednt\": \"need not\",\n",
    "        \"letd\": \"let us\"\n",
    "    }\n",
    "    \n",
    "    # Here i am converting the review to string and then converting them to lowercase using in-built function \n",
    "    review = str(review).lower()\n",
    "    # Here using the Beautiful Soup HTML.PARSER i am extracting only text data from the reviews discarding the html tag or elements.\n",
    "    review = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "    # I am using a simple regex to remove all the URLs from the text.\n",
    "    review = re.sub(r'http\\S+|www\\S+', '', review)\n",
    "    # Here we are going through the contradiction dictionary and replacing each of the contradiction with their expanded form if they are found in the review. \n",
    "    for contraction, expanded in contractions_dict.items():\n",
    "        review = review.replace(contraction, expanded)\n",
    "    # Here i am removing all the non-alphabetic characters execpt for the spaces. This step needs to be done after the contradiction expansion as this will result in the ' getting removed also, we can cause issues when detecting the contradictions.\n",
    "    review = re.sub(r'[^a-zA-Z\\s]', '', review)\n",
    "    # Here i am coverting all the multiple consecutive spaces with single space using a regex.\n",
    "    review = re.sub(r'\\s+', ' ', review)\n",
    "    # I am returning the processed review after removing the trailing and leading space if any are remaining.\n",
    "    return review.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of reviews before cleaning: 342.90\n",
      "The average length of reviews after cleaning: 326.14\n"
     ]
    }
   ],
   "source": [
    "# Here I am calculating the average length of the review before I apply the data cleaning function\n",
    "avg_length_of_review_before_data_cleaning = sampled_df[\"review_body\"].str.len().mean()\n",
    "\n",
    "# Here I am applying the Data Cleaning function to each review in our dataframe\n",
    "sampled_df[\"review_body\"] = sampled_df[\"review_body\"].apply(clean_review)\n",
    "\n",
    "# Here I am calculating the average length of the review after I applied the data cleaning function\n",
    "avg_length_of_review_after_data_cleaning = sampled_df[\"review_body\"].str.len().mean()\n",
    "\n",
    "print(f'The average length of reviews before cleaning: {avg_length_of_review_before_data_cleaning:.2f}')\n",
    "print(f'The average length of reviews after cleaning: {avg_length_of_review_after_data_cleaning:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Spandan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "# In this function I am removing all the stopword present in the reviews, based on the NLTK stopwords list.\n",
    "def remove_stopwords(review):\n",
    "    # I am spliting the review into a list of word based on single space as separator\n",
    "    words = review.split()\n",
    "    # I am filtering out all the stop words and only including the ones which are not present in our stopwords list.\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    # I am joining all the filtered word using a single space.\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "sampled_df[\"review_body\"] = sampled_df[\"review_body\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# In this function, I am performing lemmatization on the filtered words present in our reviews using the NLTK WordNetLemmatizer.\n",
    "def perfom_lemmatization(review):\n",
    "    # I am spliting the review into a list of word based on single space as separator\n",
    "    words = review.split()\n",
    "    # Here I am considering all the words as verbs and converting them to their root form.\n",
    "    lemm_words = [lemmatizer.lemmatize(word, pos=\"v\") for word in words]\n",
    "     # I am joining all the converted word using a single space.\n",
    "    return ' '.join(lemm_words)\n",
    "\n",
    "sampled_df[\"review_body\"] = sampled_df[\"review_body\"].apply(perfom_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Sample reviews before cleaning: \n",
      "                                                                                                    review_body  \\\n",
      "0  Four Stars These are perfect for &#34;the guys&#34; to enjoy while the ladies are attending the baby shower!   \n",
      "1                              Five Stars Works great for late night private reading without disturbing others.   \n",
      "2                                   great price an no problems price was very good. I haven't had any problems.   \n",
      "\n",
      "   senti_label  \n",
      "0            1  \n",
      "1            1  \n",
      "2            1  \n",
      " \n",
      "###############################################################################################\n",
      " \n",
      "The Sample reviews after cleaning: \n",
      "                                                           review_body  \\\n",
      "0                four star perfect guy enjoy ladies attend baby shower   \n",
      "1  five star work great late night private read without disturb others   \n",
      "2                             great price problems price good problems   \n",
      "\n",
      "   senti_label  \n",
      "0            1  \n",
      "1            1  \n",
      "2            1  \n"
     ]
    }
   ],
   "source": [
    "# These are the sample reviews after I have performed the data cleaning and preprocessing steps.\n",
    "\n",
    "sample_reviews_after_cleaning = sampled_df.head(3)\n",
    "print('The Sample reviews before cleaning: ')\n",
    "print(sample_reviews_before_cleaning)\n",
    "print(\" \")\n",
    "print(\"###############################################################################################\")\n",
    "print(\" \")\n",
    "print('The Sample reviews after cleaning: ')\n",
    "print(sample_reviews_after_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of reviews before data preprocessing: 326.14\n",
      "The average length of reviews after data preprocessing: 199.40\n"
     ]
    }
   ],
   "source": [
    "# Here is the average length of reviews after I have performed the data cleaning and preprocessing steps. As we can see there is a significant reducation in the average length. Thus, concluding we have remove all the unimportant word, retaining only the ones which are important for our classification.\n",
    "\n",
    "avg_length_of_review_after_data_cleaning_and_preprocessing = sampled_df[\"review_body\"].str.len().mean()\n",
    "\n",
    "print(f'The average length of reviews before data preprocessing: {avg_length_of_review_after_data_cleaning:.2f}')\n",
    "print(f'The average length of reviews after data preprocessing: {avg_length_of_review_after_data_cleaning_and_preprocessing:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Here i am using the TfidfVectorizer to get the TF-IDF feature matrix of reviews\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Here i am extracting the TF-IDF features from each of the reviews and getting a sparse TF-IDF feature matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(sampled_df['review_body'])\n",
    "\n",
    "# Here i am converting the sparse matrix into a Pandas dataframe with the column name are the 1-grams which are considered.\n",
    "tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Here i am concatinating the TF-IDF sparase dataframe with the senti_label to get our final dataset.\n",
    "final_df = pd.concat([tfidf_df, sampled_df[\"senti_label\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For the input features i am using all the TF-IDF features found in our dataset.\n",
    "X = final_df[[col for col in final_df.columns if col != \"senti_label\"]]\n",
    "# For the output feature we have the senti_label which we derieved from the start_rating \n",
    "y = final_df['senti_label']    \n",
    "\n",
    "# Here i am doing the train test split with ratios 80:20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 160000 entries, 14917 to 150445\n",
      "Columns: 126785 entries, aa to zzzzzzz\n",
      "dtypes: Sparse[float64, 0](126785)\n",
      "memory usage: 46.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Here as you can see we have a total of 126785 input features as TF-IDF features.\n",
    "print(X_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy => Train: 0.9363, Test: 0.8923\n",
      "Precision => Train: 0.9427, Test: 0.9008\n",
      "Recall => Train: 0.9292, Test: 0.8803\n",
      "F1 Score => Train: 0.9359, Test: 0.8904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Here I am using percepton model with hyperparamter tuning of increasing the number of epoch to 10000 and enabling early stopping if the accuracy doesn't increase.\n",
    "clf = Perceptron(max_iter=10000, early_stopping=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pref = clf.predict(X_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_train_pref)\n",
    "precision_train = precision_score(y_train, y_train_pref)\n",
    "recall_train = recall_score(y_train, y_train_pref)\n",
    "f1_train = f1_score(y_train, y_train_pref)\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "precision_test = precision_score(y_test, y_pred)\n",
    "recall_test = recall_score(y_test, y_pred)\n",
    "f1_test = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy => Train: {accuracy_train:.4f}, Test: {accuracy_test:.4f}')\n",
    "print(f'Precision => Train: {precision_train:.4f}, Test: {precision_test:.4f}')\n",
    "print(f'Recall => Train: {recall_train:.4f}, Test: {recall_test:.4f}')\n",
    "print(f'F1 Score => Train: {f1_train:.4f}, Test: {f1_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy => Train: 0.9566, Test: 0.9204\n",
      "Precision => Train: 0.9581, Test: 0.9213\n",
      "Recall => Train: 0.9550, Test: 0.9183\n",
      "F1 Score => Train: 0.9566, Test: 0.9198\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Here I am using LinearSVC model with hyperparamter tuning of increasing the number of epoch to 10000. I am using LinearSVC here instead of SVC due to the large size of data. In the Sklearn documentation, it is given that using SVC may be impratical beyond the tens of thousands of samples, and LinearSVC is preferred for larger datasets.\n",
    "clf = LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pref = clf.predict(X_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_train_pref)\n",
    "precision_train = precision_score(y_train, y_train_pref)\n",
    "recall_train = recall_score(y_train, y_train_pref)\n",
    "f1_train = f1_score(y_train, y_train_pref)\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "precision_test = precision_score(y_test, y_pred)\n",
    "recall_test = recall_score(y_test, y_pred)\n",
    "f1_test = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy => Train: {accuracy_train:.4f}, Test: {accuracy_test:.4f}')\n",
    "print(f'Precision => Train: {precision_train:.4f}, Test: {precision_test:.4f}')\n",
    "print(f'Recall => Train: {recall_train:.4f}, Test: {recall_test:.4f}')\n",
    "print(f'F1 Score => Train: {f1_train:.4f}, Test: {f1_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy => Train: 0.9324, Test: 0.9228\n",
      "Precision => Train: 0.9360, Test: 0.9252\n",
      "Recall => Train: 0.9286, Test: 0.9191\n",
      "F1 Score => Train: 0.9323, Test: 0.9222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Here I am using Logistic Rgression model with hyperparamter tuning of increasing the number of epoch to 10000 and using the solver algorithm as liblinear, since it is giving very slightly better test accuracy than default lbfgs.\n",
    "clf = LogisticRegression(solver='liblinear', max_iter=10000)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pref = clf.predict(X_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_train_pref)\n",
    "precision_train = precision_score(y_train, y_train_pref)\n",
    "recall_train = recall_score(y_train, y_train_pref)\n",
    "f1_train = f1_score(y_train, y_train_pref)\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "precision_test = precision_score(y_test, y_pred)\n",
    "recall_test = recall_score(y_test, y_pred)\n",
    "f1_test = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy => Train: {accuracy_train:.4f}, Test: {accuracy_test:.4f}')\n",
    "print(f'Precision => Train: {precision_train:.4f}, Test: {precision_test:.4f}')\n",
    "print(f'Recall => Train: {recall_train:.4f}, Test: {recall_test:.4f}')\n",
    "print(f'F1 Score => Train: {f1_train:.4f}, Test: {f1_test:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy => Train: 0.8934, Test: 0.8765\n",
      "Precision => Train: 0.9168, Test: 0.8975\n",
      "Recall => Train: 0.8657, Test: 0.8484\n",
      "F1 Score => Train: 0.8905, Test: 0.8723\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Here i am using the default Multinomial Naive Bayes model without any hyperparameter tuning.\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pref = clf.predict(X_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, y_train_pref)\n",
    "precision_train = precision_score(y_train, y_train_pref)\n",
    "recall_train = recall_score(y_train, y_train_pref)\n",
    "f1_train = f1_score(y_train, y_train_pref)\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "precision_test = precision_score(y_test, y_pred)\n",
    "recall_test = recall_score(y_test, y_pred)\n",
    "f1_test = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy => Train: {accuracy_train:.4f}, Test: {accuracy_test:.4f}')\n",
    "print(f'Precision => Train: {precision_train:.4f}, Test: {precision_test:.4f}')\n",
    "print(f'Recall => Train: {recall_train:.4f}, Test: {recall_test:.4f}')\n",
    "print(f'F1 Score => Train: {f1_train:.4f}, Test: {f1_test:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
