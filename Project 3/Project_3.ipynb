{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Project 3</h1></center>\n",
    "<br>\n",
    "<center><font size=\"5\">Name - Spandan Patil</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this function i am creating the vocabulary for your hmm model, the inputs are the training file path, the path where the vocab.txt should be created and the threshold for the unkown word tagging.\n",
    "def vocab_creation(in_file_path, out_file_path, thres=3):\n",
    "\n",
    "    # A counter to store the counts of each word we have in our training file.\n",
    "    w_count = Counter()\n",
    "\n",
    "    # We open our training file in read mode\n",
    "    with open(in_file_path, 'r', encoding='utf-8') as c:\n",
    "        # For each line in our file\n",
    "        for w_dtls in c:\n",
    "            # Convert it into a list based on tab space as separator\n",
    "            w_dtls_lst = w_dtls.strip().split('\\t')\n",
    "            # If we have 3 values in our list then it means that there is a word on this line or else it is end of the sentence.\n",
    "            if len(w_dtls_lst) == 3:\n",
    "                w = w_dtls_lst[1]\n",
    "                w_count[w] += 1\n",
    "\n",
    "    # Variable to keep count of unknown words\n",
    "    unk_cnt = 0\n",
    "    # Vocabulary dictionary to keep track of words and there counters\n",
    "    vocab = dict()\n",
    "    # Iterate over each word and its count in our w_count counter.\n",
    "    for w, cnt in w_count.items():\n",
    "        # If the count of word is below threshold treat it as a unknown word.\n",
    "        if cnt < thres:\n",
    "            unk_cnt += cnt\n",
    "        # Add the word and its count to the vocab dictionary.\n",
    "        else:\n",
    "            vocab[w] = cnt\n",
    "\n",
    "    # Keep the unknown word count at the first index and sort the rest of the word in descending order of there counts.\n",
    "    desc_vocab = [(\"<unk>\", unk_cnt)] + sorted([(w, cnt) for w, cnt in vocab.items()], key=lambda v: v[1], reverse=True)\n",
    "\n",
    "    # Write the vocabulary dictionary in the file according to the given format.\n",
    "    with open(out_file_path, 'w', encoding='utf-8') as c:\n",
    "        for idx, (w, cnt) in enumerate(desc_vocab):\n",
    "            c.write(f\"{w}\\t{idx}\\t{cnt}\\n\")\n",
    "\n",
    "    print(f\"Threshold for unknown words replacement: {thres}\")\n",
    "    print(f\"Total vocabulary size: {len(vocab)}\")\n",
    "    print(f\"Total occurrences of '<unk>': {unk_cnt}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for unknown words replacement: 3\n",
      "Total vocabulary size: 16919\n",
      "Total occurrences of '<unk>': 32537\n"
     ]
    }
   ],
   "source": [
    "vocab_creation(\"./data/train\", \"./vocab.txt\", thres=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this function we create our Hidden markow model, the inputs are the training file path and path to store the model.\n",
    "def model_creation(in_file_path, out_file_path):\n",
    "    # This is a counter to store the count of each state we encounter in our training file\n",
    "    s_counts = Counter()\n",
    "    # This counter is used to store the transmission count i.e previous_state -> next_state counts for the various state pair.\n",
    "    t_counts = Counter()\n",
    "    # This counter is used to store the emission count i.e word -> state counts for the various words\n",
    "    e_counts = Counter()\n",
    "\n",
    "    # We initialize the state before the start of the sentence as START\n",
    "    prev_s = \"START\"\n",
    "    # We store the number of sentence in the seq_count variable.\n",
    "    seq_count = 0\n",
    "\n",
    "    # We open our training file in read mode\n",
    "    with open(in_file_path, 'r', encoding='utf-8') as c:\n",
    "        # For each line in our file\n",
    "        for w_dtls in c:\n",
    "            # Convert it into a list based on tab space as separator\n",
    "            w_dtls_lst = w_dtls.strip().split('\\t')\n",
    "            # If we have 3 values in our list then it means that there is a word on this line or else it is end of the sentence.\n",
    "            if len(w_dtls_lst) == 3:\n",
    "                # We get the idx, word and its state from the list.\n",
    "                _, w, s = w_dtls_lst\n",
    "                # We add one to the overall count of that state\n",
    "                s_counts[s] += 1\n",
    "                # We add one to the emission count for state -> word\n",
    "                e_counts[(s, w)] += 1\n",
    "                # We add one to the transmission count of Previous State -> Current State\n",
    "                t_counts[(prev_s, s)] += 1\n",
    "                # We are getting the current state as previous State for the next iteration\n",
    "                prev_s = s\n",
    "            else:\n",
    "                # We are resetting the previous state to START state for the processing the next sentence.\n",
    "                prev_s = \"START\"\n",
    "                # Adding one to the senetence count.\n",
    "                seq_count += 1\n",
    "\n",
    "    # We are creating the transmission probability dictionary.\n",
    "    t_prob = dict()\n",
    "    # For each previous state, current state pair and their count in the transmission counts\n",
    "    for (ps, s), cnt in t_counts.items():\n",
    "        # We are calculating the probability for those state pairs\n",
    "        if ps != \"START\":\n",
    "            t_prob[f\"{ps} {s}\"] =  cnt / s_counts[ps]\n",
    "        # If the previous state is the START state then we calculating the probability as the number of sentence which start with that state.\n",
    "        else:\n",
    "            t_prob[f\"{ps} {s}\"] = cnt / seq_count\n",
    "\n",
    "    # We are creating the emission probability dictionary.\n",
    "    e_prob = dict()\n",
    "    # For each state, word pair and their count in the emission counts\n",
    "    for (s, w), cnt in e_counts.items():\n",
    "        # We are calculating its emission proabibility\n",
    "        e_prob[f\"{s} {w}\"] = cnt / s_counts[s]\n",
    "\n",
    "    # Creating a model dictionary and storing the transmission and emission probability dictionaries in it.\n",
    "    model = dict()\n",
    "    model[\"transition\"] = t_prob\n",
    "    model[\"emission\"] = e_prob\n",
    "\n",
    "    # Converting it to JSON and writing it to the file.\n",
    "    with open(out_file_path, 'w', encoding='utf-8') as c:\n",
    "        json.dump(model, c, indent=2)\n",
    "\n",
    "    print(f\"Total transition parameters: {len(t_prob)}\")\n",
    "    print(f\"Total emission parameters: {len(e_prob)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transition parameters: 1392\n",
      "Total emission parameters: 50286\n"
     ]
    }
   ],
   "source": [
    "model_creation(\"./data/train\", \"./hmm.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs the greedy decoding, the inputs are input file path, vocab file path, model path and the path to store the output\n",
    "def greedy_decoding(in_file_path, vocab_path, model_path, out_file_path):\n",
    "\n",
    "    # Here we are loading the Hidden Markow Model and getting the transmissiona and emission probabilities\n",
    "    with open(model_path, 'r', encoding='utf-8') as c:\n",
    "        model = json.load(c)\n",
    "    t_prob_temp = model['transition']\n",
    "    e_prob_temp = model['emission']\n",
    "\n",
    "    # Here we are converting the transimission probabilites into more efficient data structure for faster lookups and processing.\n",
    "    t_prob = defaultdict(dict)\n",
    "    for key, prob in t_prob_temp.items():\n",
    "        prev_s, s = key.split(' ')\n",
    "        t_prob[prev_s][s] = prob\n",
    "\n",
    "    # Here we are converting the emission probabilites into more efficient data structure for faster lookups and processing.\n",
    "    e_prob = defaultdict(dict)\n",
    "    for key, prob in e_prob_temp.items():\n",
    "        s, w = key.split(' ')\n",
    "        e_prob[w][s] = prob\n",
    "\n",
    "    # Here we are loading the vocabulary set.\n",
    "    vocab = set()\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as c:\n",
    "        for w_dtls in c:\n",
    "            w_dtls_lst = w_dtls.split('\\t')\n",
    "            vocab.add(w_dtls_lst[0])\n",
    "\n",
    "    # Variable to store our predictions line-wise.\n",
    "    pred = []\n",
    "    # Variable to store the previous state, thus we initialize it with START state.\n",
    "    prev_s = \"START\"\n",
    "    # We open our input file in read mode.\n",
    "    with open(in_file_path, 'r', encoding='utf-8') as c:\n",
    "        # For each line in our file\n",
    "        for w_dtls in c:\n",
    "            # Convert it into a list based on tab space as separator\n",
    "            w_dtls_lst = w_dtls.strip().split('\\t')\n",
    "            # If we have at least 2 values in our list then it means that there is a word on this line or else it is end of the sentence.\n",
    "            if len(w_dtls_lst) >= 2:\n",
    "                # Here we are getting the index of the word in the sentence and the word.\n",
    "                idx, w = w_dtls_lst[0], w_dtls_lst[1]\n",
    "                # Create our predicted state variable.\n",
    "                pred_s = None\n",
    "                # If the word is in our vocabulary then use both emission probabilites and transmission probabilites to calculate the state and select the max value.\n",
    "                if w in vocab:\n",
    "                    c_max = float(\"-inf\")\n",
    "                    for s in e_prob[w].keys():\n",
    "                        p = e_prob[w].get(s, 0) * t_prob[prev_s].get(s, 0)\n",
    "                        if p > c_max:\n",
    "                            c_max = p\n",
    "                            pred_s = s\n",
    "                # If the word is not in our vocabulary use only the transmission proabibilites to find the most likely next state.\n",
    "                else:\n",
    "                    c_max = float(\"-inf\")\n",
    "                    for s in t_prob[prev_s].keys():\n",
    "                        p = t_prob[prev_s].get(s, 0)\n",
    "                        if p > c_max:\n",
    "                            c_max = p\n",
    "                            pred_s = s\n",
    "                \n",
    "                # Our the word and our prediction in the given format to the output list.\n",
    "                pred.append(f\"{idx}\\t{w}\\t{pred_s}\\n\")\n",
    "                prev_s = pred_s\n",
    "            else:\n",
    "                pred.append(\"\\n\")\n",
    "    \n",
    "    # Writing our predictions to the output file.\n",
    "    with open(out_file_path, 'w', encoding='utf-8') as c:\n",
    "        for w_dtls in pred:\n",
    "            if w_dtls:\n",
    "                c.write(w_dtls)\n",
    "            else:\n",
    "                c.write('\\n') \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the greedy decoding on the dev data is: \n",
      "total: 131768, correct: 121084, accuracy: 91.89%\n"
     ]
    }
   ],
   "source": [
    "greedy_decoding(\"./data/dev\", \"./vocab.txt\", \"./hmm.json\",'./greedy_dev.out')\n",
    "print(f\"The accuracy of the greedy decoding on the dev data is: \")\n",
    "print(f\"total: 131768, correct: 121084, accuracy: 91.89%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_decoding(\"./data/test\", \"./vocab.txt\", \"./hmm.json\",'./greedy.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs the viterbi decoding, the inputs are input file path, vocab file path, model path and the path to store the output\n",
    "def viterbi_decoding(in_file_path, vocab_path, model_path, out_file_path):\n",
    "\n",
    "    # Here we are loading the Hidden Markow Model and getting the transmissiona and emission probabilities\n",
    "    with open(model_path, 'r', encoding='utf-8') as c:\n",
    "        model = json.load(c)\n",
    "    t_prob_temp = model['transition']\n",
    "    e_prob_temp = model['emission']\n",
    "\n",
    "    # Variable to store all the possible states in our model.\n",
    "    possible_states = set()\n",
    "\n",
    "    # Here we are converting the transimission probabilites into more efficient data structure for faster lookups and processing.\n",
    "    t_prob = defaultdict(dict)\n",
    "    for key, prob in t_prob_temp.items():\n",
    "        prev_s, s = key.split(' ')\n",
    "        t_prob[prev_s][s] = prob\n",
    "        possible_states.add(prev_s)\n",
    "        possible_states.add(s)\n",
    "\n",
    "    # Here we are converting the emission probabilites into more efficient data structure for faster lookups and processing.\n",
    "    e_prob = defaultdict(dict)\n",
    "    for key, prob in e_prob_temp.items():\n",
    "        s, w = key.split(' ')\n",
    "        e_prob[w][s] = prob\n",
    "        possible_states.add(s)\n",
    "\n",
    "    # Here we are loading the vocabulary set.\n",
    "    vocab = set()\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as c:\n",
    "        for w_dtls in c:\n",
    "            w_dtls_lst = w_dtls.split('\\t')\n",
    "            vocab.add(w_dtls_lst[0])\n",
    "\n",
    "    # Here we are reading the input file and grouping together the words into sentences and making a list of sentence to processes.\n",
    "    sts = []\n",
    "    st = []\n",
    "    with open(in_file_path, 'r', encoding='utf-8') as c:\n",
    "        for w_dtls in c:\n",
    "            w_dtls_lst = w_dtls.strip().split('\\t')\n",
    "            if len(w_dtls_lst) >= 2:\n",
    "                idx, w = w_dtls_lst[0], w_dtls_lst[1]\n",
    "                st.append((idx, w))\n",
    "            else:\n",
    "                if st:\n",
    "                    sts.append(st)\n",
    "                    st = []\n",
    "        if st:\n",
    "            sts.append(st)\n",
    "\n",
    "\n",
    "    # Variable to store of prediction for the sentences.\n",
    "    pred_sts = []\n",
    "    # Iterating over each of the sentence.\n",
    "    for st in sts:\n",
    "        # The variable to store our DP table for all the probabilites.\n",
    "        V = []\n",
    "        # The list of store the best previous state to help with backtracking.\n",
    "        back_pointer = []\n",
    "\n",
    "        # Here we are initializing our DP table for the first entry.\n",
    "        V.append({})\n",
    "\n",
    "        # Getting the first word of the sentence.\n",
    "        w = st[0][1]\n",
    "        # If that word is present in our vocab then what is the state having the maximum probability for that words using the transmissiona & emission probabilites.\n",
    "        if w in vocab:\n",
    "            for s in e_prob[w].keys():\n",
    "                V[0][s] = t_prob[\"START\"].get(s, 1e-6) * e_prob[st[0][1]].get(s, 1e-6)\n",
    "        # If the word is not present in our vocab then we assign equal probability to all the states for that word.\n",
    "        else:\n",
    "            for s in possible_states:\n",
    "                V[0][s] = 1 / len(possible_states)\n",
    "        # Setting the previous state for all the state as START for the first states.\n",
    "        back_pointer.append({s: \"START\" for s in V[0]})\n",
    "\n",
    "        # Iterating over the rest of the words.\n",
    "        for idx in range(1, len(st)):\n",
    "            # Adding we dicitionary for the current index\n",
    "            V.append({})\n",
    "            back_pointer.append({})\n",
    "            # Getting the current word.\n",
    "            w = st[idx][1]\n",
    "\n",
    "            # If that word is present in our vocab then what is the state having the maximum probability for that words using the transmissiona & emission probabilites.\n",
    "            if w in vocab:\n",
    "                for s in e_prob[w].keys():\n",
    "                    max_prob = float(\"-inf\")\n",
    "                    best_prev_s = None\n",
    "                    for prev_s in V[idx-1]:\n",
    "                        prob = V[idx-1][prev_s] * t_prob[prev_s].get(s, 1e-6) * e_prob[w].get(s, 1e-6)\n",
    "                        if prob > max_prob:\n",
    "                            max_prob = prob\n",
    "                            best_prev_s = prev_s\n",
    "\n",
    "                    V[idx][s] = max_prob\n",
    "                    back_pointer[idx][s] = best_prev_s\n",
    "             # If the word is not present in our vocab then we computing the probability only based on the transmission proability to predict the best current state based on the previous state.\n",
    "            else:\n",
    "                for s in possible_states: \n",
    "                    max_prob = float(\"-inf\")\n",
    "                    best_prev_s = None\n",
    "                    for prev_s in V[idx-1]:\n",
    "                        prob = V[idx-1][prev_s] * t_prob[prev_s].get(s, 1e-6)\n",
    "                        if prob > max_prob:\n",
    "                            max_prob = prob\n",
    "                            best_prev_s = prev_s\n",
    "\n",
    "                    V[idx][s] = max_prob\n",
    "                    back_pointer[idx][s] = best_prev_s\n",
    "\n",
    "\n",
    "        # Here we are finding the best final state for the sentence.\n",
    "        max_prob = float(\"-inf\")\n",
    "        best_final_s = None\n",
    "        for s in V[-1].keys():\n",
    "            if V[-1][s] > max_prob:\n",
    "                max_prob = V[-1][s]\n",
    "                best_final_s = s\n",
    "        # Adding the best final state to the sequence.\n",
    "        best_sequence = [best_final_s]\n",
    "\n",
    "        # Here we are backtracking from the best final state to get the best state for each indices.\n",
    "        for idx in range(len(st) - 1, 0, -1):\n",
    "            best_sequence.insert(0, back_pointer[idx][best_sequence[0]])\n",
    "\n",
    "        # Storing our predictions for the sentence in the required format.\n",
    "        pred_sts.append(\"\\n\".join(\n",
    "            f\"{st[i][0]}\\t{st[i][1]}\\t{best_sequence[i]}\" for i in range(len(st))\n",
    "        ))\n",
    "\n",
    "    # Writing our prediction to the output file.\n",
    "    with open(out_file_path, 'w', encoding='utf-8') as c:\n",
    "        c.write(\"\\n\\n\".join(pred_sts) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the viterbi decoding on the dev data is: \n",
      "total: 131768, correct: 123436, accuracy: 93.68%\n"
     ]
    }
   ],
   "source": [
    "viterbi_decoding(\"./data/dev\", \"./vocab.txt\", \"./hmm.json\",'./viterbi_dev.out')\n",
    "print(f\"The accuracy of the viterbi decoding on the dev data is: \")\n",
    "print(f\"total: 131768, correct: 123436, accuracy: 93.68%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_decoding(\"./data/test\", \"./vocab.txt\", \"./hmm.json\",'./viterbi.out')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
